
@article{barak2017CurrentOpinioninNeurobiology_RecurrentNeuralNetworks,
  title = {Recurrent Neural Networks as Versatile Tools of Neuroscience Research},
  author = {Barak, Omri},
  year = {2017},
  month = oct,
  journal = {Current Opinion in Neurobiology},
  series = {Computational {{Neuroscience}}},
  volume = {46},
  pages = {1--6},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2017.06.003},
  abstract = {Recurrent neural networks (RNNs) are a class of computational models that are often used as a tool to explain neurobiological phenomena, considering anatomical, electrophysiological and computational constraints. RNNs can either be designed to implement a certain dynamical principle, or they can be trained by input\textendash output examples. Recently, there has been large progress in utilizing trained RNNs both for computational tasks, and as explanations of neural phenomena. I will review how combining trained RNNs with reverse engineering can provide an alternative framework for modeling in neuroscience, potentially serving as a powerful hypothesis generation tool. Despite the recent progress and potential benefits, there are many fundamental gaps towards a theory of these networks. I will discuss these challenges and possible methods to attack them.},
  language = {en}
}

@article{beiran2020ArXiv200702062Q-Bio_ShapingDynamicsMultiple,
  title = {Shaping Dynamics with Multiple Populations in Low-Rank Recurrent Networks},
  author = {Beiran, Manuel and Dubreuil, Alexis and Valente, Adrian and Mastrogiuseppe, Francesca and Ostojic, Srdjan},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.02062 [q-bio]},
  eprint = {2007.02062},
  eprinttype = {arxiv},
  primaryclass = {q-bio},
  abstract = {An emerging paradigm proposes that neural computations can be understood at the level of dynamical systems that govern low-dimensional trajectories of collective neural activity. How the connectivity structure of a network determines the emergent dynamical system however remains to be clarified. Here we consider a novel class of models, Gaussian-mixture low-rank recurrent networks, in which the rank of the connectivity matrix and the number of statistically-defined populations are independent hyper-parameters. We show that the resulting collective dynamics form a dynamical system, where the rank sets the dimensionality and the population structure shapes the dynamics. In particular, the collective dynamics can be described in terms of a simplified effective circuit of interacting latent variables. While having a single, global population strongly restricts the possible dynamics, we demonstrate that if the number of populations is large enough, a rank \$R\$ network can approximate any \$R\$-dimensional dynamical system.},
  archiveprefix = {arXiv},
  keywords = {Quantitative Biology - Neurons and Cognition}
}

@book{dayan2001_TheoreticalNeuroscienceComputational,
  title = {Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems},
  shorttitle = {Theoretical Neuroscience},
  author = {Dayan, Peter and Abbott, Laurence F.},
  year = {2001}
}

@article{depasquale2018PLOSONE_FullFORCETargetbasedMethod,
  title = {Full-{{FORCE}}: {{A}} Target-Based Method for Training Recurrent Networks},
  shorttitle = {Full-{{FORCE}}},
  author = {DePasquale, Brian and Cueva, Christopher J. and Rajan, Kanaka and Escola, G. Sean and Abbott, L. F.},
  year = {2018},
  month = feb,
  journal = {PLOS ONE},
  volume = {13},
  number = {2},
  pages = {e0191527},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0191527},
  abstract = {Trained recurrent networks are powerful tools for modeling dynamic neural computations. We present a target-based method for modifying the full connectivity matrix of a recurrent network to train it to perform tasks involving temporally complex input/output transformations. The method introduces a second network during training to provide suitable ``target'' dynamics useful for performing the task. Because it exploits the full recurrent connectivity, the method produces networks that perform tasks with fewer neurons and greater noise robustness than traditional least-squares (FORCE) approaches. In addition, we show how introducing additional input signals into the target-generating network, which act as task hints, greatly extends the range of tasks that can be learned and provides control over the complexity and nature of the dynamics of the trained, task-performing network.},
  language = {en},
  keywords = {Algorithms,Eigenvalues,Learning,Machine learning algorithms,Network analysis,Neural networks,Neuroscience,Signaling networks}
}

@article{ganguli2008Neuron_OneDimensionalDynamicsAttention,
  title = {One-{{Dimensional Dynamics}} of {{Attention}} and {{Decision Making}} in {{LIP}}},
  author = {Ganguli, Surya and Bisley, James W. and Roitman, Jamie D. and Shadlen, Michael N. and Goldberg, Michael E. and Miller, Kenneth D.},
  year = {2008},
  month = apr,
  journal = {Neuron},
  volume = {58},
  number = {1},
  pages = {15--25},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2008.01.038},
  abstract = {Where we allocate our visual spatial attention depends upon a continual competition between internally generated goals and external distractions. Recently it was shown that single neurons in the macaque lateral intraparietal area (LIP) can predict the amount of time a distractor can shift the locus of spatial attention away from a goal. We propose that this remarkable dynamical correspondence between single neurons and attention can be explained by a network model in which generically high-dimensional firing-rate vectors rapidly decay to a single mode. We find direct experimental evidence for this model, not only in the original attentional task, but also in a very different task involving perceptual decision making. These results confirm a theoretical prediction that slowly varying activity patterns are proportional to spontaneous activity, pose constraints on models of persistent activity, and suggest a network mechanism for the emergence of robust behavioral timing from heterogeneous neuronal populations.},
  language = {en},
  keywords = {SYSNEURO}
}

@article{ganguli2008PNAS_MemoryTracesDynamical,
  title = {Memory Traces in Dynamical Systems},
  author = {Ganguli, Surya and Huh, Dongsung and Sompolinsky, Haim},
  year = {2008},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {105},
  number = {48},
  pages = {18970--18975},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0804451105},
  abstract = {To perform nontrivial, real-time computations on a sensory input stream, biological systems must retain a short-term memory trace of their recent inputs. It has been proposed that generic high-dimensional dynamical systems could retain a memory trace for past inputs in their current state. This raises important questions about the fundamental limits of such memory traces and the properties required of dynamical systems to achieve these limits. We address these issues by applying Fisher information theory to dynamical systems driven by time-dependent signals corrupted by noise. We introduce the Fisher Memory Curve (FMC) as a measure of the signal-to-noise ratio (SNR) embedded in the dynamical state relative to the input SNR. The integrated FMC indicates the total memory capacity. We apply this theory to linear neuronal networks and show that the capacity of networks with normal connectivity matrices is exactly 1 and that of any network of N neurons is, at most, N. A nonnormal network achieving this bound is subject to stringent design constraints: It must have a hidden feedforward architecture that superlinearly amplifies its input for a time of order N, and the input connectivity must optimally match this architecture. The memory capacity of networks subject to saturating nonlinearities is further limited, and cannot exceed N--{$\surd$}N{$<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="i1" overflow="scroll"{$><$}mml:mrow{$><$}mml:msqrt{$><$}mml:mi{$>$}N{$<$}/mml:mi{$><$}/mml:msqrt{$><$}/mml:mrow{$><$}/mml:math{$>$}. This limit can be realized by feedforward structures with divergent fan out that distributes the signal across neurons, thereby avoiding saturation. We illustrate the generality of the theory by showing that memory in fluid systems can be sustained by transient nonnormal amplification due to convective instability or the onset of turbulence.},
  chapter = {Physical Sciences},
  copyright = {\textcopyright{} 2008 by The National Academy of Sciences of the USA},
  language = {en},
  pmid = {19020074},
  keywords = {Fisher information,fluid mechanics,network dynamics}
}

@article{ganguli2009Neuron_FeedforwardRelationNeuronal,
  title = {Feedforward to the {{Past}}: {{The Relation}} between {{Neuronal Connectivity}}, {{Amplification}}, and {{Short}}-{{Term Memory}}},
  shorttitle = {Feedforward to the {{Past}}},
  author = {Ganguli, Surya and Latham, Peter},
  year = {2009},
  month = feb,
  journal = {Neuron},
  volume = {61},
  number = {4},
  pages = {499--501},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2009.02.006},
  abstract = {Two studies in this issue of Neuron challenge widely held assumptions about the role of positive feedback in recurrent neuronal networks. Goldman shows that such feedback is not necessary for memory maintenance in a neural integrator, and Murphy and Miller show that it is not necessary for amplification of orientation patterns in V1. Both suggest that seemingly recurrent networks can be feedforward in disguise.},
  language = {en}
}

@article{goldman2009Neuron_MemoryFeedbackNeural,
  title = {Memory without {{Feedback}} in a {{Neural Network}}},
  author = {Goldman, Mark S.},
  year = {2009},
  month = feb,
  journal = {Neuron},
  volume = {61},
  number = {4},
  pages = {621--634},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2008.12.012},
  abstract = {Memory storage on short timescales is thought to be maintained by neuronal activity that persists after the remembered stimulus is removed. Although previous work suggested that positive feedback is necessary to maintain persistent activity, here it is demonstrated how neuronal responses can instead be maintained by a purely feedforward mechanism in which activity is passed sequentially through a chain of network states. This feedforward form of memory storage is shown to occur both in architecturally feedforward networks and in recurrent networks that nevertheless function in a feedforward manner. The networks can be tuned to be perfect integrators of their inputs or to reproduce the time-varying firing patterns observed during some working memory tasks but not easily reproduced by feedback-based attractor models. This work illustrates a mechanism for maintaining short-term memory in which both feedforward and feedback processes interact to govern network behavior.},
  language = {en},
  keywords = {SYSNEURO}
}

@article{hennequin2012Phys.Rev.E_NonnormalAmplificationRandom,
  title = {Non-Normal Amplification in Random Balanced Neuronal Networks},
  author = {Hennequin, Guillaume and Vogels, Tim P. and Gerstner, Wulfram},
  year = {2012},
  month = jul,
  journal = {Physical Review E},
  volume = {86},
  number = {1},
  pages = {011909},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.86.011909},
  abstract = {In dynamical models of cortical networks, the recurrent connectivity can amplify the input given to the network in two distinct ways. One is induced by the presence of near-critical eigenvalues in the connectivity matrix W, producing large but slow activity fluctuations along the corresponding eigenvectors (dynamical slowing). The other relies on W not being normal, which allows the network activity to make large but fast excursions along specific directions. Here we investigate the trade-off between non-normal amplification and dynamical slowing in the spontaneous activity of large random neuronal networks composed of excitatory and inhibitory neurons. We use a Schur decomposition of W to separate the two amplification mechanisms. Assuming linear stochastic dynamics, we derive an exact expression for the expected amount of purely non-normal amplification. We find that amplification is very limited if dynamical slowing must be kept weak. We conclude that, to achieve strong transient amplification with little slowing, the connectivity must be structured. We show that unidirectional connections between neurons of the same type together with reciprocal connections between neurons of different types, allow for amplification already in the fast dynamical regime. Finally, our results also shed light on the differences between balanced networks in which inhibition exactly cancels excitation and those where inhibition dominates.}
}

@article{hennequin2014Neuron_OptimalControlTransient,
  title = {Optimal {{Control}} of {{Transient Dynamics}} in {{Balanced Networks Supports Generation}} of {{Complex Movements}}},
  author = {Hennequin, Guillaume and Vogels, Tim P. and Gerstner, Wulfram},
  year = {2014},
  month = jun,
  journal = {Neuron},
  volume = {82},
  number = {6},
  pages = {1394--1406},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2014.04.045},
  abstract = {Populations of neurons in motor cortex engage in complex transient dynamics of large amplitude during the execution of limb movements. Traditional network models with stochastically assigned synapses cannot reproduce this behavior. Here we introduce a class of cortical architectures with strong and random excitatory recurrence that is stabilized by intricate, fine-tuned inhibition, optimized from a~control theory perspective. Such networks transiently amplify specific activity states and can be used to reliably execute multidimensional movement patterns. Similar to the experimental observations, these transients must be preceded by a steady-state initialization phase from which the network relaxes back into the background state by way of complex internal dynamics. In our networks, excitation and inhibition are as tightly balanced as recently reported in experiments across several brain areas, suggesting inhibitory control of complex excitatory recurrence as a generic organizational principle in cortex.},
  language = {en}
}

@article{hennequin2018Neuron_DynamicalRegimeSensory,
  title = {The {{Dynamical Regime}} of {{Sensory Cortex}}: {{Stable Dynamics}} around a {{Single Stimulus}}-{{Tuned Attractor Account}} for {{Patterns}} of {{Noise Variability}}},
  shorttitle = {The {{Dynamical Regime}} of {{Sensory Cortex}}},
  author = {Hennequin, Guillaume and Ahmadian, Yashar and Rubin, Daniel B. and Lengyel, M{\'a}t{\'e} and Miller, Kenneth D.},
  year = {2018},
  month = may,
  journal = {Neuron},
  volume = {98},
  number = {4},
  pages = {846-860.e5},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2018.04.017},
  abstract = {Correlated variability in cortical activity is ubiquitously quenched following stimulus onset, in a stimulus-dependent manner. These modulations have been attributed to circuit dynamics involving either multiple stable states (``attractors'') or chaotic activity. Here we show that a qualitatively different dynamical regime, involving fluctuations about a single, stimulus-driven attractor in a loosely balanced excitatory-inhibitory network (the stochastic ``stabilized supralinear network''), best explains these modulations. Given the supralinear input/output functions of cortical neurons, increased stimulus drive strengthens effective network connectivity. This shifts the balance from interactions that amplify~variability to suppressive inhibitory feedback, quenching correlated variability around more strongly driven steady states. Comparing to previously published and original data analyses, we show that this mechanism, unlike previous proposals, uniquely accounts for the spatial patterns and fast temporal dynamics of variability suppression. Specifying the cortical operating regime is key~to understanding the computations underlying perception.},
  language = {en},
  keywords = {circuit dynamics,cortical variability,MT,noise correlations,theoretical neuroscience,V1,variability quenching}
}

@inproceedings{hennequin2019CosyneWorkshopDataDyn.Comput.UsingData-DrivenMethodsGroundMech.Theory_NeuroscienceOutControl,
  title = {Neuroscience out of Control: Control-Theoretic Methods for Understanding Neural Circuits},
  booktitle = {Cosyne {{Workshop}} ``{{Data}}, Dynamics and Computation: Using Data-Driven Methods to Ground Mechanistic Theory''},
  author = {Hennequin, Guillaume},
  year = {2019},
  month = mar,
  address = {{Lisbon, Portugal}}
}

@article{jaeger2001BonnGer.Ger.Natl.Res.Cent.Inf.Technol.GMDTech.Rep._EchoStateApproach,
  title = {The ``Echo State'' Approach to Analysing and Training Recurrent Neural Networks-with an Erratum Note},
  author = {Jaeger, Herbert},
  year = {2001},
  journal = {Bonn, Germany: German National Research Center for Information Technology GMD Technical Report},
  volume = {148},
  number = {34},
  pages = {13},
  keywords = {reservoir computing}
}

@article{kao2019CurrentOpinioninNeurobiology_NeuroscienceOutControl,
  title = {Neuroscience out of Control: Control-Theoretic Perspectives on Neural Circuit Dynamics},
  shorttitle = {Neuroscience out of Control},
  author = {Kao, Ta-Chu and Hennequin, Guillaume},
  year = {2019},
  month = oct,
  journal = {Current Opinion in Neurobiology},
  series = {Computational {{Neuroscience}}},
  volume = {58},
  pages = {122--129},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2019.09.001},
  abstract = {A major challenge in systems neuroscience is to understand how the dynamics of neural circuits give rise to behaviour. Analysis of complex dynamical systems is also at the heart of control engineering, where it is central to the design of robust control strategies. Although a rich engineering literature has grown over decades to facilitate the analysis of such systems, little of it has percolated into neuroscience so far. Here, we give a brief introduction to a number of core control-theoretic concepts that provide useful perspectives on neural circuit dynamics. We introduce important mathematical tools related to these concepts, and establish connections to neural circuit analysis, focusing on a number of themes that have arisen from the modern `state-space' view on neural population dynamics.},
  language = {en}
}

@article{logiaco2019bioRxiv_ModelFlexibleMotor,
  title = {A Model of Flexible Motor Sequencing through Thalamic Control of Cortical Dynamics},
  author = {Logiaco, Laureline and Abbott, L. F. and Escola, Sean},
  year = {2019},
  month = dec,
  journal = {bioRxiv},
  pages = {2019.12.17.880153},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2019.12.17.880153},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}The mechanisms by which neural circuits generate an extensible library of motor motifs and flexibly string them into arbitrary sequences are unclear. We developed a model in which inhibitory basal ganglia output neurons project to thalamic units that are themselves bidirectionally connected to a recurrent cortical network. During movement sequences, electrophysiological recordings of basal ganglia output neurons show sustained activity patterns that switch at the boundaries between motifs. Thus, we model these inhibitory patterns as silencing some thalamic neurons while leaving others disinhibited and free to interact with cortex during specific motifs. We show that a small number of disinhibited thalamic neurons can control cortical dynamics to generate specific motor output in a noise robust way. If the thalamic units associated with each motif are segregated, many motor outputs can be learned without interference and then combined in arbitrary orders for the flexible production of long and complex motor sequences.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  language = {en}
}

@article{logiaco2020ArXiv200613332CsQ-BioStat_ThalamocorticalMotorCircuit,
  title = {Thalamocortical Motor Circuit Insights for More Robust Hierarchical Control of Complex Sequences},
  author = {Logiaco, Laureline and Escola, G. Sean},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.13332 [cs, q-bio, stat]},
  eprint = {2006.13332},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio, stat},
  abstract = {We study learning of recurrent neural networks that produce temporal sequences consisting of the concatenation of re-usable "motifs". In the context of neuroscience or robotics, these motifs would be the motor primitives from which complex behavior is generated. Given a known set of motifs, can a new motif be learned without affecting the performance of the known set and then used in new sequences without first explicitly learning every possible transition? Two requirements enable this: (i) parameter updates while learning a new motif do not interfere with the parameters used for the previously acquired ones; and (ii) a new motif can be robustly generated when starting from the network state reached at the end of any of the other motifs, even if that state was not present during training. We meet the first requirement by investigating artificial neural networks (ANNs) with specific architectures, and attempt to meet the second by training them to generate motifs from random initial states. We find that learning of single motifs succeeds but that sequence generation is not robust: transition failures are observed. We then compare these results with a model whose architecture and analytically-tractable dynamics are inspired by the motor thalamocortical circuit, and that includes a specific module used to implement motif transitions. The synaptic weights of this model can be adjusted without requiring stochastic gradient descent (SGD) on the simulated network outputs, and we have asymptotic guarantees that transitions will not fail. Indeed, in simulations, we achieve single-motif accuracy on par with the previously studied ANNs and have improved sequencing robustness with no transition failures. Finally, we show that insights obtained by studying the transition subnetwork of this model can also improve the robustness of transitioning in the traditional ANNs previously studied.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning}
}

@article{logiaco2021CellReports_ThalamicControlCortical,
  title = {Thalamic Control of Cortical Dynamics in a Model of Flexible Motor Sequencing},
  author = {Logiaco, Laureline and Abbott, L. F. and Escola, Sean},
  year = {2021},
  month = jun,
  journal = {Cell Reports},
  volume = {35},
  number = {9},
  pages = {109090},
  issn = {2211-1247},
  doi = {10.1016/j.celrep.2021.109090},
  abstract = {The neural mechanisms that generate an extensible library of motor motifs and flexibly string them into arbitrary sequences are unclear. We developed a model in which inhibitory basal ganglia output neurons project to thalamic units that are themselves bidirectionally connected to a recurrent cortical network. We model the basal ganglia inhibitory patterns as silencing some thalamic neurons while leaving others disinhibited and free to interact with cortex during specific motifs. We show that a small number of disinhibited thalamic neurons can control cortical dynamics to generate specific motor output in a noise-robust way. Additionally, a single ``preparatory'' thalamocortical network can produce fast cortical dynamics that support rapid transitions between any pair of learned motifs. If the thalamic units associated with each sequence component are segregated, many motor outputs can be learned without interference and then combined in arbitrary orders for the flexible production of long and complex motor sequences.},
  language = {en},
  keywords = {control,hierarchical behaviors,low-rank connectivity perturbation,motor cortex,motor sequencing,recurrent neural networks,switching linear dynamics,thalamocortical loops,thalamus}
}

@article{marschall2020J.Mach.Learn.Res._UnifiedFrameworkOnline,
  title = {A {{Unified Framework}} of {{Online Learning Algorithms}} for {{Training Recurrent Neural Networks}}},
  author = {Marschall, Owen and Cho, Kyunghyun and Savin, Cristina},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  number = {135},
  pages = {1--34},
  issn = {1533-7928}
}

@article{mastrogiuseppe2017PLOSComputationalBiology_IntrinsicallygeneratedFluctuatingActivity,
  title = {Intrinsically-Generated Fluctuating Activity in Excitatory-Inhibitory Networks},
  author = {Mastrogiuseppe, Francesca and Ostojic, Srdjan},
  year = {2017},
  month = apr,
  journal = {PLOS Computational Biology},
  volume = {13},
  number = {4},
  pages = {e1005498},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005498},
  abstract = {Recurrent networks of non-linear units display a variety of dynamical regimes depending on the structure of their synaptic connectivity. A particularly remarkable phenomenon is the appearance of strongly fluctuating, chaotic activity in networks of deterministic, but randomly connected rate units. How this type of intrinsically generated fluctuations appears in more realistic networks of spiking neurons has been a long standing question. To ease the comparison between rate and spiking networks, recent works investigated the dynamical regimes of randomly-connected rate networks with segregated excitatory and inhibitory populations, and firing rates constrained to be positive. These works derived general dynamical mean field (DMF) equations describing the fluctuating dynamics, but solved these equations only in the case of purely inhibitory networks. Using a simplified excitatory-inhibitory architecture in which DMF equations are more easily tractable, here we show that the presence of excitation qualitatively modifies the fluctuating activity compared to purely inhibitory networks. In presence of excitation, intrinsically generated fluctuations induce a strong increase in mean firing rates, a phenomenon that is much weaker in purely inhibitory networks. Excitation moreover induces two different fluctuating regimes: for moderate overall coupling, recurrent inhibition is sufficient to stabilize fluctuations; for strong coupling, firing rates are stabilized solely by the upper bound imposed on activity, even if inhibition is stronger than excitation. These results extend to more general network architectures, and to rate networks receiving noisy inputs mimicking spiking activity. Finally, we show that signatures of the second dynamical regime appear in networks of integrate-and-fire neurons.},
  language = {en},
  keywords = {Action potentials,Dynamical systems,Eigenvalues,Neural networks,Neurons,Stochastic processes,Transfer functions,White noise}
}

@article{mastrogiuseppe2018Neuron_LinkingConnectivityDynamics,
  title = {Linking {{Connectivity}}, {{Dynamics}}, and {{Computations}} in {{Low}}-{{Rank Recurrent Neural Networks}}},
  author = {Mastrogiuseppe, Francesca and Ostojic, Srdjan},
  year = {2018},
  month = aug,
  journal = {Neuron},
  volume = {99},
  number = {3},
  pages = {609-623.e29},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2018.07.003},
  abstract = {Large-scale neural recordings have established that the transformation of sensory stimuli into motor outputs relies on low-dimensional dynamics at the population level, while individual neurons exhibit complex selectivity. Understanding how low-dimensional computations on mixed, distributed representations emerge from the structure of the recurrent connectivity and inputs to cortical networks is a major challenge. Here, we study a class of recurrent network models in which the connectivity is a sum of a random part and a minimal, low-dimensional structure. We show that, in such networks, the dynamics are low dimensional and can be directly inferred from connectivity using a geometrical approach. We exploit this understanding to determine minimal connectivity required to implement specific computations and find that the dynamical range and computational capacity quickly increase with the dimensionality of the connectivity structure. This framework produces testable experimental predictions for the relationship between connectivity, low-dimensional dynamics, and computational features of recorded neurons.},
  language = {en},
  keywords = {low dimensional dynamics,mixed selectivity,neural computations,recurrent neural networks}
}

@article{miller2011NeuralComputation_MathematicalEquivalenceTwo,
  title = {Mathematical {{Equivalence}} of {{Two Common Forms}} of {{Firing Rate Models}} of {{Neural Networks}}},
  author = {Miller, Kenneth D. and Fumarola, Francesco},
  year = {2011},
  month = dec,
  journal = {Neural Computation},
  volume = {24},
  number = {1},
  pages = {25--31},
  publisher = {{MIT Press}},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00221},
  abstract = {We demonstrate the mathematical equivalence of two commonly used forms of firing rate model equations for neural networks. In addition, we show that what is commonly interpreted as the firing rate in one form of model may be better interpreted as a low-pass-filtered firing rate, and we point out a conductance-based firing rate model.}
}

@article{morales2021ArXiv210105848Nlin_UnveilingRolePlasticity,
  title = {Unveiling the Role of Plasticity Rules in Reservoir Computing},
  author = {Morales, Guillermo B. and Mirasso, Claudio R. and Soriano, Miguel C.},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.05848 [nlin]},
  eprint = {2101.05848},
  eprinttype = {arxiv},
  primaryclass = {nlin},
  abstract = {Reservoir Computing (RC) is an appealing approach in Machine Learning that combines the high computational capabilities of Recurrent Neural Networks with a fast and easy training method. Likewise, successful implementation of neuro-inspired plasticity rules into RC artificial networks has boosted the performance of the original models. In this manuscript, we analyze the role that plasticity rules play on the changes that lead to a better performance of RC. To this end, we implement synaptic and non-synaptic plasticity rules in a paradigmatic example of RC model: the Echo State Network. Testing on nonlinear time series prediction tasks, we show evidence that improved performance in all plastic models are linked to a decrease of the pair-wise correlations in the reservoir, as well as a significant increase of individual neurons ability to separate similar inputs in their activity space. Here we provide new insights on this observed improvement through the study of different stages on the plastic learning. From the perspective of the reservoir dynamics, optimal performance is found to occur close to the so-called edge of instability. Our results also show that it is possible to combine different forms of plasticity (namely synaptic and non-synaptic rules) to further improve the performance on prediction tasks, obtaining better results than those achieved with single-plasticity models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Nonlinear Sciences - Adaptation and Self-Organizing Systems}
}

@article{murphy2009Neuron_BalancedAmplificationNew,
  title = {Balanced {{Amplification}}: {{A New Mechanism}} of {{Selective Amplification}} of {{Neural Activity Patterns}}},
  shorttitle = {Balanced {{Amplification}}},
  author = {Murphy, Brendan K. and Miller, Kenneth D.},
  year = {2009},
  month = feb,
  journal = {Neuron},
  volume = {61},
  number = {4},
  pages = {635--648},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2009.02.005},
  abstract = {In cerebral cortex, ongoing activity absent a stimulus can resemble stimulus-driven activity in size and structure. In particular, spontaneous activity in cat primary visual cortex (V1) has structure significantly correlated with evoked responses to oriented stimuli. This suggests that, from unstructured input, cortical circuits selectively amplify specific activity patterns. Current understanding of selective amplification involves elongation of a neural assembly's lifetime by mutual excitation among its neurons. We introduce a new mechanism for selective amplification without elongation of lifetime: ``balanced amplification.'' Strong balanced amplification arises when feedback inhibition stabilizes strong recurrent excitation, a pattern likely to be typical of cortex. Thus, balanced amplification should ubiquitously~contribute to cortical activity. Balanced amplification depends on the fact that individual neurons project only excitatory or only inhibitory synapses. This leads to a hidden feedforward connectivity between activity patterns. We show in a detailed biophysical model that this can explain the cat V1 observations.},
  language = {en},
  keywords = {SYSNEURO}
}

@article{rajan2006Phys.Rev.Lett._EigenvalueSpectraRandom,
  title = {Eigenvalue {{Spectra}} of {{Random Matrices}} for {{Neural Networks}}},
  author = {Rajan, Kanaka and Abbott, L. F.},
  year = {2006},
  month = nov,
  journal = {Physical Review Letters},
  volume = {97},
  number = {18},
  pages = {188104},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevLett.97.188104},
  abstract = {The dynamics of neural networks is influenced strongly by the spectrum of eigenvalues of the matrix describing their synaptic connectivity. In large networks, elements of the synaptic connectivity matrix can be chosen randomly from appropriate distributions, making results from random matrix theory highly relevant. Unfortunately, classic results on the eigenvalue spectra of random matrices do not apply to synaptic connectivity matrices because of the constraint that individual neurons are either excitatory or inhibitory. Therefore, we compute eigenvalue spectra of large random matrices with excitatory and inhibitory columns drawn from distributions with different means and equal or different variances.}
}

@article{rajan2010Phys.Rev.E_StimulusdependentSuppressionChaos,
  title = {Stimulus-Dependent Suppression of Chaos in Recurrent Neural Networks},
  author = {Rajan, Kanaka and Abbott, L. F. and Sompolinsky, Haim},
  year = {2010},
  month = jul,
  journal = {Physical Review E},
  volume = {82},
  number = {1},
  pages = {011903},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.82.011903},
  abstract = {Neuronal activity arises from an interaction between ongoing firing generated spontaneously by neural circuits and responses driven by external stimuli. Using mean-field analysis, we ask how a neural network that intrinsically generates chaotic patterns of activity can remain sensitive to extrinsic input. We find that inputs not only drive network responses, but they also actively suppress ongoing activity, ultimately leading to a phase transition in which chaos is completely eliminated. The critical input intensity at the phase transition is a nonmonotonic function of stimulus frequency, revealing a ``resonant'' frequency at which the input is most effective at suppressing chaos even though the power spectrum of the spontaneous activity peaks at zero and falls exponentially. A prediction of our analysis is that the variance of neural responses should be most strongly suppressed at frequencies matching the range over which many sensory systems operate.}
}

@article{schuessler2020ArXiv200611036Q-Bio_InterplayRandomnessStructure,
  title = {The Interplay between Randomness and Structure during Learning in {{RNNs}}},
  author = {Schuessler, Friedrich and Mastrogiuseppe, Francesca and Dubreuil, Alexis and Ostojic, Srdjan and Barak, Omri},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.11036 [q-bio]},
  eprint = {2006.11036},
  eprinttype = {arxiv},
  primaryclass = {q-bio},
  abstract = {Recurrent neural networks (RNNs) trained on low-dimensional tasks have been widely used to model functional biological networks. However, the solutions found by learning and the effect of initial connectivity are not well understood. Here, we examine RNNs trained using gradient descent on different tasks inspired by the neuroscience literature. We find that the changes in recurrent connectivity can be described by low-rank matrices, despite the unconstrained nature of the learning algorithm. To identify the origin of the low-rank structure, we turn to an analytically tractable setting: training a linear RNN on a simplified task. We show how the low-dimensional task structure leads to low-rank changes to connectivity. This low-rank structure allows us to explain and quantify the phenomenon of accelerated learning in the presence of random initial connectivity. Altogether, our study opens a new perspective to understanding trained RNNs in terms of both the learning process and the resulting network structure.},
  archiveprefix = {arXiv},
  keywords = {Quantitative Biology - Neurons and Cognition}
}

@article{schuessler2020Phys.Rev.Research_DynamicsRandomRecurrent,
  title = {Dynamics of Random Recurrent Networks with Correlated Low-Rank Structure},
  author = {Schuessler, Friedrich and Dubreuil, Alexis and Mastrogiuseppe, Francesca and Ostojic, Srdjan and Barak, Omri},
  year = {2020},
  month = feb,
  journal = {Physical Review Research},
  volume = {2},
  number = {1},
  pages = {013111},
  doi = {10.1103/PhysRevResearch.2.013111},
  abstract = {A given neural network in the brain is involved in many different tasks. This implies that, when considering a specific task, the network's connectivity contains a component which is related to the task and another component which can be considered random. Understanding the interplay between the structured and random components and their effect on network dynamics and functionality is an important open question. Recent studies addressed the coexistence of random and structured connectivity but considered the two parts to be uncorrelated. This constraint limits the dynamics and leaves the random connectivity nonfunctional. Algorithms that train networks to perform specific tasks typically generate correlations between structure and random connectivity. Here we study nonlinear networks with correlated structured and random components, assuming the structure to have a low rank. We develop an analytic framework to establish the precise effect of the correlations on the eigenvalue spectrum of the joint connectivity. We find that the spectrum consists of a bulk and multiple outliers, whose location is predicted by our theory. Using mean-field theory, we show that these outliers directly determine both the fixed points of the system and their stability. Taken together, our analysis elucidates how correlations allow structured and random connectivity to synergistically extend the range of computations available to networks.}
}

@article{seung1996PNAS_HowBrainKeeps,
  title = {How the Brain Keeps the Eyes Still},
  author = {Seung, H. S.},
  year = {1996},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {93},
  number = {23},
  pages = {13339--13344},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.93.23.13339},
  abstract = {{$<$}p{$>$}The brain can hold the eyes still because it stores a memory of eye position. The brain's memory of horizontal eye position appears to be represented by persistent neural activity in a network known as the neural integrator, which is localized in the brainstem and cerebellum. Existing experimental data are reinterpreted as evidence for an ``attractor hypothesis'' that the persistent patterns of activity observed in this network form an attractive line of fixed points in its state space. Line attractor dynamics can be produced in linear or nonlinear neural networks by learning mechanisms that precisely tune positive feedback.{$<$}/p{$>$}},
  chapter = {Biological Sciences},
  copyright = {Copyright \textcopyright{} 1996, The National Academy of Sciences of the USA},
  language = {en},
  pmid = {8917592}
}

@article{sohn2019Neuron_BayesianComputationCortical,
  title = {Bayesian {{Computation}} through {{Cortical Latent Dynamics}}},
  author = {Sohn, Hansem and Narain, Devika and Meirhaeghe, Nicolas and Jazayeri, Mehrdad},
  year = {2019},
  month = sep,
  journal = {Neuron},
  volume = {103},
  number = {5},
  pages = {934-947.e5},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2019.06.012},
  abstract = {Statistical regularities in the environment create prior beliefs that we rely on to optimize our behavior when sensory information is uncertain. Bayesian theory formalizes how prior beliefs can be leveraged and has had a major impact on models of perception, sensorimotor function, and cognition. However, it is not known how recurrent interactions among neurons mediate Bayesian integration. By using a time-interval reproduction task in monkeys, we found that prior statistics warp neural representations in the frontal cortex, allowing the mapping of sensory inputs to motor outputs to incorporate prior statistics in accordance with Bayesian inference. Analysis of recurrent neural network models performing the task revealed that this warping was enabled by a low-dimensional curved manifold and allowed us to further probe the potential causal underpinnings of this computational strategy. These results uncover a simple and general principle whereby prior beliefs exert their influence on behavior by sculpting cortical latent dynamics.},
  language = {en},
  keywords = {Bayesian inference,Bayesian integration,frontal cortex,neural manifold,neural trajectories,recurrent neural networks}
}

@article{sompolinsky1988Phys.Rev.Lett._ChaosRandomNeural,
  title = {Chaos in {{Random Neural Networks}}},
  author = {Sompolinsky, H. and Crisanti, A. and Sommers, H. J.},
  year = {1988},
  month = jul,
  journal = {Physical Review Letters},
  volume = {61},
  number = {3},
  pages = {259--262},
  doi = {10.1103/PhysRevLett.61.259},
  abstract = {A continuous-time dynamic model of a network of N nonlinear elements interacting via random asymmetric couplings is studied. A self-consistent mean-field theory, exact in the N\textrightarrow{$\infty$} limit, predicts a transition from a stationary phase to a chaotic phase occurring at a critical value of the gain parameter. The autocorrelations of the chaotic flow as well as the maximal Lyapunov exponent are calculated.}
}

@article{song2016PLOSComputationalBiology_TrainingExcitatoryInhibitoryRecurrent,
  title = {Training {{Excitatory}}-{{Inhibitory Recurrent Neural Networks}} for {{Cognitive Tasks}}: {{A Simple}} and {{Flexible Framework}}},
  shorttitle = {Training {{Excitatory}}-{{Inhibitory Recurrent Neural Networks}} for {{Cognitive Tasks}}},
  author = {Song, H. Francis and Yang, Guangyu R. and Wang, Xiao-Jing},
  year = {2016},
  month = feb,
  journal = {PLOS Computational Biology},
  volume = {12},
  number = {2},
  pages = {e1004792},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004792},
  abstract = {The ability to simultaneously record from large numbers of neurons in behaving animals has ushered in a new era for the study of the neural circuit mechanisms underlying cognitive functions. One promising approach to uncovering the dynamical and computational principles governing population responses is to analyze model recurrent neural networks (RNNs) that have been optimized to perform the same tasks as behaving animals. Because the optimization of network parameters specifies the desired output but not the manner in which to achieve this output, ``trained'' networks serve as a source of mechanistic hypotheses and a testing ground for data analyses that link neural computation to behavior. Complete access to the activity and connectivity of the circuit, and the ability to manipulate them arbitrarily, make trained networks a convenient proxy for biological circuits and a valuable platform for theoretical investigation. However, existing RNNs lack basic biological features such as the distinction between excitatory and inhibitory units (Dale's principle), which are essential if RNNs are to provide insights into the operation of biological circuits. Moreover, trained networks can achieve the same behavioral performance but differ substantially in their structure and dynamics, highlighting the need for a simple and flexible framework for the exploratory training of RNNs. Here, we describe a framework for gradient descent-based training of excitatory-inhibitory RNNs that can incorporate a variety of biological knowledge. We provide an implementation based on the machine learning library Theano, whose automatic differentiation capabilities facilitate modifications and extensions. We validate this framework by applying it to well-known experimental paradigms such as perceptual decision-making, context-dependent integration, multisensory integration, parametric working memory, and motor sequence generation. Our results demonstrate the wide range of neural activity patterns and behavior that can be modeled, and suggest a unified setting in which diverse cognitive computations and mechanisms can be studied.},
  language = {en},
  keywords = {Decision making,Machine learning,Neural networks,Neural pathways,Neurons,Reaction time,Sensory perception,Working memory}
}

@article{stroud2018Nat.Neurosci._MotorPrimitivesSpace,
  title = {Motor Primitives in Space and Time via Targeted Gain Modulation in Cortical Networks},
  author = {Stroud, Jake P. and Porter, Mason A. and Hennequin, Guillaume and Vogels, Tim P.},
  year = {2018},
  month = dec,
  journal = {Nature Neuroscience},
  volume = {21},
  number = {12},
  pages = {1774--1783},
  issn = {1546-1726},
  doi = {10.1038/s41593-018-0276-0},
  abstract = {Many behavioral tasks require fast, reliable switching of the shape and duration of cortical activity. Stroud et al. show that modulation of neural excitability in recurrent network models provides flexible spatiotemporal control of neural activity.},
  copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  language = {en},
  keywords = {neuroscience}
}

@article{susman2021Phys.Rev.Research_QualityInternalRepresentation,
  title = {Quality of Internal Representation Shapes Learning Performance in Feedback Neural Networks},
  author = {Susman, Lee and Mastrogiuseppe, Francesca and Brenner, Naama and Barak, Omri},
  year = {2021},
  month = feb,
  journal = {Physical Review Research},
  volume = {3},
  number = {1},
  pages = {013176},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevResearch.3.013176},
  abstract = {A fundamental feature of complex biological systems is the ability to form feedback interactions with their environment. A prominent model for studying such interactions is reservoir computing, where learning acts on low-dimensional bottlenecks. Despite the simplicity of this learning scheme, the factors contributing to or hindering the success of training in reservoir networks are in general not well understood. In this work, we study nonlinear feedback networks trained to generate a sinusoidal signal, and analyze how learning performance is shaped by the interplay between internal network dynamics and target properties. By performing exact mathematical analysis of linearized networks, we predict that learning performance is maximized when the target is characterized by an optimal, intermediate frequency which monotonically decreases with the strength of the internal reservoir connectivity. At the optimal frequency, the reservoir representation of the target signal is high-dimensional, desynchronized, and thus maximally robust to noise. We show that our predictions successfully capture the qualitative behavior of performance in nonlinear networks. Moreover, we find that the relationship between internal representations and performance can be further exploited in trained nonlinear networks to explain behaviors which do not have a linear counterpart. Our results indicate that a major determinant of learning success is the quality of the internal representation of the target, which in turn is shaped by an interplay between parameters controlling the internal network and those defining the task.}
}

@article{sussillo2009Neuron_GeneratingCoherentPatterns,
  title = {Generating {{Coherent Patterns}} of {{Activity}} from {{Chaotic Neural Networks}}},
  author = {Sussillo, David and Abbott, L. F.},
  year = {2009},
  month = aug,
  journal = {Neuron},
  volume = {63},
  number = {4},
  pages = {544--557},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2009.07.018},
  abstract = {Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated.},
  language = {en},
  keywords = {SYSNEURO}
}

@article{sussillo2015Nat.Neurosci._NeuralNetworkThat,
  title = {A Neural Network That Finds a Naturalistic Solution for the Production of Muscle Activity},
  author = {Sussillo, David and Churchland, Mark M. and Kaufman, Matthew T. and Shenoy, Krishna V.},
  year = {2015},
  month = jul,
  journal = {Nature Neuroscience},
  volume = {18},
  number = {7},
  pages = {1025--1033},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4042},
  abstract = {How motor cortical activity relates to muscle movement is still unclear. Here the authors trained neural networks to reproduce muscle activity of reaching monkeys. The optimal solutions produced by these networks resembled the single-neuron and population level neural activity seen in the motor cortex of the same monkeys.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  language = {en}
}

@article{wang2018Nat.Neurosci._PrefrontalCortexMetareinforcement,
  title = {Prefrontal Cortex as a Meta-Reinforcement Learning System},
  author = {Wang, Jane X. and {Kurth-Nelson}, Zeb and Kumaran, Dharshan and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Hassabis, Demis and Botvinick, Matthew},
  year = {2018},
  month = jun,
  journal = {Nature Neuroscience},
  volume = {21},
  number = {6},
  pages = {860--868},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-018-0147-8},
  abstract = {Over the past 20 years, neuroscience research on reward-based learning has converged on a canonical model, under which the neurotransmitter dopamine `stamps in' associations between situations, actions and rewards by modulating the strength of synaptic connections between neurons. However, a growing number of recent findings have placed this standard model under strain. We now draw on recent advances in artificial intelligence to introduce a new theory of reward-based learning. Here, the dopamine system trains another part of the brain, the prefrontal cortex, to operate as its own free-standing learning system. This new perspective accommodates the findings that motivated the standard model, but also deals gracefully with a wider range of observations, providing a fresh foundation for future research.},
  copyright = {2018 The Author(s)},
  language = {en}
}

@article{warnberg2019PLOSComputationalBiology_PerturbingLowDimensional,
  title = {Perturbing Low Dimensional Activity Manifolds in Spiking Neuronal Networks},
  author = {W{\"a}rnberg, Emil and Kumar, Arvind},
  year = {2019},
  month = may,
  journal = {PLOS Computational Biology},
  volume = {15},
  number = {5},
  pages = {e1007074},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1007074},
  abstract = {Several recent studies have shown that neural activity in vivo tends to be constrained to a low-dimensional manifold. Such activity does not arise in simulated neural networks with homogeneous connectivity and it has been suggested that it is indicative of some other connectivity pattern in neuronal networks. In particular, this connectivity pattern appears to be constraining learning so that only neural activity patterns falling within the intrinsic manifold can be learned and elicited. Here, we use three different models of spiking neural networks (echo-state networks, the Neural Engineering Framework and Efficient Coding) to demonstrate how the intrinsic manifold can be made a direct consequence of the circuit connectivity. Using this relationship between the circuit connectivity and the intrinsic manifold, we show that learning of patterns outside the intrinsic manifold corresponds to much larger changes in synaptic weights than learning of patterns within the intrinsic manifold. Assuming larger changes to synaptic weights requires extensive learning, this observation provides an explanation of why learning is easier when it does not require the neural activity to leave its intrinsic manifold.},
  language = {en},
  keywords = {Action potentials,Coding mechanisms,Dynamical systems,Engineering and technology,Neural networks,Neurons,Permutation,Synapses}
}

@article{white2004Phys.Rev.Lett._ShortTermMemoryOrthogonal,
  title = {Short-{{Term Memory}} in {{Orthogonal Neural Networks}}},
  author = {White, Olivia L. and Lee, Daniel D. and Sompolinsky, Haim},
  year = {2004},
  month = apr,
  journal = {Physical Review Letters},
  volume = {92},
  number = {14},
  pages = {148102},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevLett.92.148102},
  abstract = {We study the ability of linear recurrent networks obeying discrete time dynamics to store long temporal sequences that are retrievable from the instantaneous state of the network. We calculate this temporal memory capacity for both distributed shift register and random orthogonal connectivity matrices. We show that the memory capacity of these networks scales with system size.}
}

@article{williamson2016PLoSComputBiol_ScalingPropertiesDimensionality,
  title = {Scaling {{Properties}} of {{Dimensionality Reduction}} for {{Neural Populations}} and {{Network Models}}},
  author = {Williamson, Ryan C. and Cowley, Benjamin R. and {Litwin-Kumar}, Ashok and Doiron, Brent and Kohn, Adam and Smith, Matthew A. and Yu, Byron M.},
  year = {2016},
  month = dec,
  journal = {PLoS Computational Biology},
  volume = {12},
  number = {12},
  issn = {1553-734X},
  doi = {10.1371/journal.pcbi.1005141},
  abstract = {Recent studies have applied dimensionality reduction methods to understand how the multi-dimensional structure of neural population activity gives rise to brain function. It is unclear, however, how the results obtained from dimensionality reduction generalize to recordings with larger numbers of neurons and trials or how these results relate to the underlying network structure. We address these questions by applying factor analysis to recordings in the visual cortex of non-human primates and to spiking network models that self-generate irregular activity through a balance of excitation and inhibition. We compared the scaling trends of two key outputs of dimensionality reduction\textemdash shared dimensionality and percent shared variance\textemdash with neuron and trial count. We found that the scaling properties of networks with non-clustered and clustered connectivity differed, and that the in vivo recordings were more consistent with the clustered network. Furthermore, recordings from tens of neurons were sufficient to identify the dominant modes of shared variability that generalize to larger portions of the network. These findings can help guide the interpretation of dimensionality reduction outputs in regimes of limited neuron and trial sampling and help relate these outputs to the underlying network structure., We seek to understand how billions of neurons in the brain work together to give rise to everyday brain function. In most current experimental settings, we can only record from tens of neurons for a few hours at a time. A major question in systems neuroscience is whether our interpretation of how neurons interact would change if we monitor orders of magnitude more neurons and for substantially more time. In this study, we use realistic networks of model neurons, which allow us to analyze the activity from as many model neurons as we want for as long as we want. For these models, we found that we can identify the salient interactions among neurons and interpret their activity meaningfully within the range of neurons and recording time available in current experiments. Furthermore, we studied how the neural activity from the models reflects how the neurons are connected. These results help to guide the interpretation of analyses using populations of neurons in the context of the larger network to understand brain function.},
  pmcid = {PMC5142778},
  pmid = {27926936}
}

@article{wong2006J.Neurosci._RecurrentNetworkMechanism,
  title = {A {{Recurrent Network Mechanism}} of {{Time Integration}} in {{Perceptual Decisions}}},
  author = {Wong, Kong-Fatt and Wang, Xiao-Jing},
  year = {2006},
  month = jan,
  journal = {Journal of Neuroscience},
  volume = {26},
  number = {4},
  pages = {1314--1328},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3733-05.2006},
  abstract = {Recent physiological studies using behaving monkeys revealed that, in a two-alternative forced-choice visual motion discrimination task, reaction time was correlated with ramping of spike activity of lateral intraparietal cortical neurons. The ramping activity appears to reflect temporal accumulation, on a timescale of hundreds of milliseconds, of sensory evidence before a decision is reached. To elucidate the cellular and circuit basis of such integration times, we developed and investigated a simplified two-variable version of a biophysically realistic cortical network model of decision making. In this model, slow time integration can be achieved robustly if excitatory reverberation is primarily mediated by NMDA receptors; our model with only fast AMPA receptors at recurrent synapses produces decision times that are not comparable with experimental observations. Moreover, we found two distinct modes of network behavior, in which decision computation by winner-take-all competition is instantiated with or without attractor states for working memory. Decision process is closely linked to the local dynamics, in the ``decision space'' of the system, in the vicinity of an unstable saddle steady state that separates the basins of attraction for the two alternative choices. This picture provides a rigorous and quantitative explanation for the dependence of performance and response time on the degree of task difficulty, and the reason for which reaction times are longer in error trials than in correct trials as observed in the monkey experiment. Our reduced two-variable neural model offers a simple yet biophysically plausible framework for studying perceptual decision making in general.},
  chapter = {Articles},
  copyright = {Copyright \textcopyright{} 2006 Society for Neuroscience 0270-6474/06/261314-15\$15.00/0},
  language = {en},
  pmid = {16436619},
  keywords = {computational modeling,dynamical systems,intraparietal cortex,NMDA,reaction time,sensory discrimination}
}

@article{yang2019NatNeurosci_TaskRepresentationsNeural,
  title = {Task Representations in Neural Networks Trained to Perform Many Cognitive Tasks},
  author = {Yang, Guangyu Robert and Joglekar, Madhura R. and Song, H. Francis and Newsome, William T. and Wang, Xiao-Jing},
  year = {2019},
  month = feb,
  journal = {Nature Neuroscience},
  volume = {22},
  number = {2},
  pages = {297--306},
  issn = {1546-1726},
  doi = {10.1038/s41593-018-0310-2},
  abstract = {Prefrontal cortex can be flexibly engaged in many different tasks. Yang et al. trained an artificial neural network to solve 20 cognitive tasks. Functionally specialized modules and compositional representations emerged in the network after training.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  language = {en}
}

@inproceedings{zhou2021CosyneAbstr._LearningSensoryRepresentations,
  title = {Learning Sensory Representations for Flexible Computation with Recurrent Circuits.},
  booktitle = {Cosyne {{Abstracts}}},
  author = {Zhou, Liang and Latham, Peter and Menendez, Jorge Aurelio},
  year = {2021}
}




@article{stroud2018Nat.Neurosci._MotorPrimitivesSpace,
  title = {Motor Primitives in Space and Time via Targeted Gain Modulation in Cortical Networks},
  volume = {21},
  copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  issn = {1546-1726},
  abstract = {Many behavioral tasks require fast, reliable switching of the shape and duration of cortical activity. Stroud et al. show that modulation of neural excitability in recurrent network models provides flexible spatiotemporal control of neural activity.},
  language = {en},
  number = {12},
  journal = {Nature Neuroscience},
  doi = {10.1038/s41593-018-0276-0},
  author = {Stroud, Jake P. and Porter, Mason A. and Hennequin, Guillaume and Vogels, Tim P.},
  month = dec,
  year = {2018},
  keywords = {neuroscience},
  pages = {1774-1783},
  file = {/nfs/nhome/live/jorgem/paper-pdfs/recurrent-networks/Stroud2018Nature_Neuroscience.pdf;/Users/jorgeamenendez/Zotero/storage/X8UKDKEA/s41593-018-0276-0.html}
}

@article{jaeger2001BonnGer.Ger.Natl.Res.Cent.Inf.Technol.GMDTech.Rep._EchoStateApproach,
  title = {The ``Echo State'' Approach to Analysing and Training Recurrent Neural Networks-with an Erratum Note},
  volume = {148},
  number = {34},
  journal = {Bonn, Germany: German National Research Center for Information Technology GMD Technical Report},
  author = {Jaeger, Herbert},
  year = {2001},
  keywords = {reservoir computing},
  pages = {13},
  file = {/nfs/nhome/live/jorgem/paper-pdfs/control-theory/Jaeger2001Bonn,_Germany_German_National_Research_Center_for_Information_Technology_GMD_Technical_Report.pdf}
}

@article{schuessler2019ArXiv190904358Q-Bio_DynamicsRandomRecurrent,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1909.04358},
  primaryClass = {q-bio},
  title = {Dynamics of Random Recurrent Networks with Correlated Low-Rank Structure},
  abstract = {A given neural network in the brain is involved in many different tasks. This implies that, when considering a specific task, the network's connectivity contains a component which is related to the task and another component which can be considered random. Understanding the interplay between the structured and random components, and their effect on network dynamics and functionality is an important open question. Recent studies addressed the co-existence of random and structured connectivity, but considered the two parts to be uncorrelated. This constraint limits the dynamics and leaves the random connectivity non-functional. Algorithms that train networks to perform specific tasks typically generate correlations between structure and random connectivity. Here we study nonlinear networks with correlated structured and random components, assuming the structure to have a low rank. We develop an analytic framework to establish the precise effect of the correlations on the eigenvalue spectrum of the joint connectivity. We find that the spectrum consists of a bulk and multiple outliers, whose location is predicted by our theory. Using mean-field theory, we show that these outliers directly determine both the fixed points of the system and their stability. Taken together, our analysis elucidates how correlations allow structured and random connectivity to synergistically extend the range of computations available to networks.},
  journal = {arXiv:1909.04358 [q-bio]},
  author = {Schuessler, Friedrich and Dubreuil, Alexis and Mastrogiuseppe, Francesca and Ostojic, Srdjan and Barak, Omri},
  month = oct,
  year = {2019},
  keywords = {random networks},
  file = {/Users/jorgeamenendez/Zotero/storage/ARX8M8DF/Schuessler et al. - 2019 - Dynamics of random recurrent networks with correla.pdf}
}

@article{depasquale2018PLOSONE_FullFORCETargetbasedMethod,
  title = {Full-{{FORCE}}: {{A}} Target-Based Method for Training Recurrent Networks},
  volume = {13},
  issn = {1932-6203},
  shorttitle = {Full-{{FORCE}}},
  abstract = {Trained recurrent networks are powerful tools for modeling dynamic neural computations. We present a target-based method for modifying the full connectivity matrix of a recurrent network to train it to perform tasks involving temporally complex input/output transformations. The method introduces a second network during training to provide suitable ``target'' dynamics useful for performing the task. Because it exploits the full recurrent connectivity, the method produces networks that perform tasks with fewer neurons and greater noise robustness than traditional least-squares (FORCE) approaches. In addition, we show how introducing additional input signals into the target-generating network, which act as task hints, greatly extends the range of tasks that can be learned and provides control over the complexity and nature of the dynamics of the trained, task-performing network.},
  language = {en},
  number = {2},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0191527},
  author = {DePasquale, Brian and Cueva, Christopher J. and Rajan, Kanaka and Escola, G. Sean and Abbott, L. F.},
  month = feb,
  year = {2018},
  keywords = {Algorithms,Eigenvalues,Learning,Machine learning algorithms,Network analysis,Neural networks,Neuroscience,Signaling networks},
  pages = {e0191527},
  file = {/Users/jorgeamenendez/Google Drive/paper-pdfs/recurrent-networks/DePasquale2018PLOS_ONE-full-FORCE.pdf;/Users/jorgeamenendez/Zotero/storage/MGYYBAHD/article.html}
}

@article{sussillo2009Neuron_GeneratingCoherentPatternsa,
  title = {Generating {{Coherent Patterns}} of {{Activity}} from {{Chaotic Neural Networks}}},
  volume = {63},
  issn = {0896-6273},
  abstract = {Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated.},
  language = {en},
  number = {4},
  journal = {Neuron},
  doi = {10.1016/j.neuron.2009.07.018},
  author = {Sussillo, David and Abbott, L. F.},
  month = aug,
  year = {2009},
  keywords = {SYSNEURO},
  pages = {544-557},
  file = {/Users/jorgeamenendez/Google Drive/paper-pdfs/recurrent-networks/SussilloAbbott2009Neuron-Generating_Coherent_Patterns_of_Activity_from_Chaotic_Neural_Networks.pdf;/Users/jorgeamenendez/Zotero/storage/DR244VKG/S0896627309005479.html}
}

@article{yang2019NatNeurosci_TaskRepresentationsNeural,
  title = {Task Representations in Neural Networks Trained to Perform Many Cognitive Tasks},
  volume = {22},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  issn = {1546-1726},
  abstract = {Prefrontal cortex can be flexibly engaged in many different tasks. Yang et al. trained an artificial neural network to solve 20 cognitive tasks. Functionally specialized modules and compositional representations emerged in the network after training.},
  language = {en},
  number = {2},
  journal = {Nature Neuroscience},
  doi = {10.1038/s41593-018-0310-2},
  author = {Yang, Guangyu Robert and Joglekar, Madhura R. and Song, H. Francis and Newsome, William T. and Wang, Xiao-Jing},
  month = feb,
  year = {2019},
  pages = {297-306},
  file = {/Users/jorgeamenendez/Google Drive/paper-pdfs/recurrent-networks/Yang2019Nat_Neurosci-Task_representations_in_neural_networks_trained_to_perform_many_cognitive_tasks.pdf;/Users/jorgeamenendez/Zotero/storage/ESYDBVKU/s41593-018-0310-2.html}
}



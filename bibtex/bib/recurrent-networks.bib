
@book{dayan2001_TheoreticalNeuroscienceComputational,
  title = {Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems},
  shorttitle = {Theoretical Neuroscience},
  author = {Dayan, Peter and Abbott, Laurence F.},
  year = {2001}
}

@article{depasquale2018PLOSONE_FullFORCETargetbasedMethod,
  title = {Full-{{FORCE}}: {{A}} Target-Based Method for Training Recurrent Networks},
  shorttitle = {Full-{{FORCE}}},
  author = {DePasquale, Brian and Cueva, Christopher J. and Rajan, Kanaka and Escola, G. Sean and Abbott, L. F.},
  year = {2018},
  month = feb,
  volume = {13},
  pages = {e0191527},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0191527},
  abstract = {Trained recurrent networks are powerful tools for modeling dynamic neural computations. We present a target-based method for modifying the full connectivity matrix of a recurrent network to train it to perform tasks involving temporally complex input/output transformations. The method introduces a second network during training to provide suitable ``target'' dynamics useful for performing the task. Because it exploits the full recurrent connectivity, the method produces networks that perform tasks with fewer neurons and greater noise robustness than traditional least-squares (FORCE) approaches. In addition, we show how introducing additional input signals into the target-generating network, which act as task hints, greatly extends the range of tasks that can be learned and provides control over the complexity and nature of the dynamics of the trained, task-performing network.},
  journal = {PLOS ONE},
  keywords = {Algorithms,Eigenvalues,Learning,Machine learning algorithms,Network analysis,Neural networks,Neuroscience,Signaling networks},
  language = {en},
  number = {2}
}

@article{hennequin2014Neuron_OptimalControlTransient,
  title = {Optimal {{Control}} of {{Transient Dynamics}} in {{Balanced Networks Supports Generation}} of {{Complex Movements}}},
  author = {Hennequin, Guillaume and Vogels, Tim P. and Gerstner, Wulfram},
  year = {2014},
  month = jun,
  volume = {82},
  pages = {1394--1406},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2014.04.045},
  abstract = {Populations of neurons in motor cortex engage in complex transient dynamics of large amplitude during the execution of limb movements. Traditional network models with stochastically assigned synapses cannot reproduce this behavior. Here we introduce a class of cortical architectures with strong and random excitatory recurrence that is stabilized by intricate, fine-tuned inhibition, optimized from a~control theory perspective. Such networks transiently amplify specific activity states and can be used to reliably execute multidimensional movement patterns. Similar to the experimental observations, these transients must be preceded by a steady-state initialization phase from which the network relaxes back into the background state by way of complex internal dynamics. In our networks, excitation and inhibition are as tightly balanced as recently reported in experiments across several brain areas, suggesting inhibitory control of complex excitatory recurrence as a generic organizational principle in cortex.},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{jaeger2001BonnGer.Ger.Natl.Res.Cent.Inf.Technol.GMDTech.Rep._EchoStateApproach,
  title = {The ``Echo State'' Approach to Analysing and Training Recurrent Neural Networks-with an Erratum Note},
  author = {Jaeger, Herbert},
  year = {2001},
  volume = {148},
  pages = {13},
  journal = {Bonn, Germany: German National Research Center for Information Technology GMD Technical Report},
  keywords = {reservoir computing},
  number = {34}
}

@article{mastrogiuseppe2018Neuron_LinkingConnectivityDynamics,
  title = {Linking {{Connectivity}}, {{Dynamics}}, and {{Computations}} in {{Low}}-{{Rank Recurrent Neural Networks}}},
  author = {Mastrogiuseppe, Francesca and Ostojic, Srdjan},
  year = {2018},
  month = aug,
  volume = {99},
  pages = {609-623.e29},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2018.07.003},
  abstract = {Large-scale neural recordings have established that the transformation of sensory stimuli into motor outputs relies on low-dimensional dynamics at the population level, while individual neurons exhibit complex selectivity. Understanding how low-dimensional computations on mixed, distributed representations emerge from the structure of the recurrent connectivity and inputs to cortical networks is a major challenge. Here, we study a class of recurrent network models in which the connectivity is a sum of a random part and a minimal, low-dimensional structure. We show that, in such networks, the dynamics are low dimensional and can be directly inferred from connectivity using a geometrical approach. We exploit this understanding to determine minimal connectivity required to implement specific computations and find that the dynamical range and computational capacity quickly increase with the dimensionality of the connectivity structure. This framework produces testable experimental predictions for the relationship between connectivity, low-dimensional dynamics, and computational features of recorded neurons.},
  journal = {Neuron},
  keywords = {low dimensional dynamics,mixed selectivity,neural computations,recurrent neural networks},
  language = {en},
  number = {3}
}

@article{schuessler2019ArXiv190904358Q-Bio_DynamicsRandomRecurrent,
  title = {Dynamics of Random Recurrent Networks with Correlated Low-Rank Structure},
  author = {Schuessler, Friedrich and Dubreuil, Alexis and Mastrogiuseppe, Francesca and Ostojic, Srdjan and Barak, Omri},
  year = {2019},
  month = oct,
  abstract = {A given neural network in the brain is involved in many different tasks. This implies that, when considering a specific task, the network's connectivity contains a component which is related to the task and another component which can be considered random. Understanding the interplay between the structured and random components, and their effect on network dynamics and functionality is an important open question. Recent studies addressed the co-existence of random and structured connectivity, but considered the two parts to be uncorrelated. This constraint limits the dynamics and leaves the random connectivity non-functional. Algorithms that train networks to perform specific tasks typically generate correlations between structure and random connectivity. Here we study nonlinear networks with correlated structured and random components, assuming the structure to have a low rank. We develop an analytic framework to establish the precise effect of the correlations on the eigenvalue spectrum of the joint connectivity. We find that the spectrum consists of a bulk and multiple outliers, whose location is predicted by our theory. Using mean-field theory, we show that these outliers directly determine both the fixed points of the system and their stability. Taken together, our analysis elucidates how correlations allow structured and random connectivity to synergistically extend the range of computations available to networks.},
  archivePrefix = {arXiv},
  eprint = {1909.04358},
  eprinttype = {arxiv},
  journal = {arXiv:1909.04358 [q-bio]},
  keywords = {random networks},
  primaryClass = {q-bio}
}

@article{sompolinsky1988Phys.Rev.Lett._ChaosRandomNeural,
  title = {Chaos in {{Random Neural Networks}}},
  author = {Sompolinsky, H. and Crisanti, A. and Sommers, H. J.},
  year = {1988},
  month = jul,
  volume = {61},
  pages = {259--262},
  doi = {10.1103/PhysRevLett.61.259},
  abstract = {A continuous-time dynamic model of a network of N nonlinear elements interacting via random asymmetric couplings is studied. A self-consistent mean-field theory, exact in the N\textrightarrow{$\infty$} limit, predicts a transition from a stationary phase to a chaotic phase occurring at a critical value of the gain parameter. The autocorrelations of the chaotic flow as well as the maximal Lyapunov exponent are calculated.},
  journal = {Physical Review Letters},
  number = {3}
}

@article{stroud2018Nat.Neurosci._MotorPrimitivesSpace,
  title = {Motor Primitives in Space and Time via Targeted Gain Modulation in Cortical Networks},
  author = {Stroud, Jake P. and Porter, Mason A. and Hennequin, Guillaume and Vogels, Tim P.},
  year = {2018},
  month = dec,
  volume = {21},
  pages = {1774--1783},
  issn = {1546-1726},
  doi = {10.1038/s41593-018-0276-0},
  abstract = {Many behavioral tasks require fast, reliable switching of the shape and duration of cortical activity. Stroud et al. show that modulation of neural excitability in recurrent network models provides flexible spatiotemporal control of neural activity.},
  copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  journal = {Nature Neuroscience},
  keywords = {neuroscience},
  language = {en},
  number = {12}
}

@article{sussillo2009Neuron_GeneratingCoherentPatterns,
  title = {Generating {{Coherent Patterns}} of {{Activity}} from {{Chaotic Neural Networks}}},
  author = {Sussillo, David and Abbott, L. F.},
  year = {2009},
  month = aug,
  volume = {63},
  pages = {544--557},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2009.07.018},
  abstract = {Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated.},
  journal = {Neuron},
  keywords = {SYSNEURO},
  language = {en},
  number = {4}
}

@article{yang2019NatNeurosci_TaskRepresentationsNeural,
  title = {Task Representations in Neural Networks Trained to Perform Many Cognitive Tasks},
  author = {Yang, Guangyu Robert and Joglekar, Madhura R. and Song, H. Francis and Newsome, William T. and Wang, Xiao-Jing},
  year = {2019},
  month = feb,
  volume = {22},
  pages = {297--306},
  issn = {1546-1726},
  doi = {10.1038/s41593-018-0310-2},
  abstract = {Prefrontal cortex can be flexibly engaged in many different tasks. Yang et al. trained an artificial neural network to solve 20 cognitive tasks. Functionally specialized modules and compositional representations emerged in the network after training.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  journal = {Nature Neuroscience},
  language = {en},
  number = {2}
}



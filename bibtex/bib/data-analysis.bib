
@article{bjorck1973Math.Comp._NumericalMethodsComputing,
  title = {Numerical Methods for Computing Angles between Linear Subspaces},
  author = {Bj{\"o}rck, Ake and Golub, Gene H.},
  year = {1973},
  journal = {Mathematics of Computation},
  volume = {27},
  number = {123},
  pages = {579--594},
  issn = {0025-5718, 1088-6842},
  doi = {10.1090/S0025-5718-1973-0348991-3},
  abstract = {Assume that two subspaces F and G of a unitary space are defined as the ranges (or null spaces) of given rectangular matrices A and B. Accurate numerical methods are developed for computing the principal angles and orthogonal sets of principal vectors and . An important application in statistics is computing the canonical correlations between two sets of variates. A perturbation analysis shows that the condition number for essentially is , where denotes the condition number of a matrix. The algorithms are based on a preliminary QR-factorization of A and B (or and ), for which either the method of Householder transformations (HT) or the modified Gram-Schmidt method (MGS) is used. Then and are computed as the singular values of certain related matrices. Experimental results are given, which indicates that MGS gives with equal precision and fewer arithmetic operations than HT. However, HT gives principal vectors, which are orthogonal to working accuracy, which is not generally true for MGS. Finally, the case when A and/or B are rank deficient is discussed.},
  language = {en},
  keywords = {canonical correlations,least squares,Numerical linear algebra,singular values}
}

@article{cunningham2014Nat.Neurosci._DimensionalityReductionLargescale,
  title = {Dimensionality Reduction for Large-Scale Neural Recordings},
  author = {Cunningham, John P. and Yu, Byron M.},
  year = {2014},
  month = nov,
  journal = {Nature Neuroscience},
  volume = {17},
  number = {11},
  pages = {1500--1509},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.3776},
  abstract = {Many recent studies have adopted dimensionality reduction to analyze neural population activity and to find features that are not apparent at the level of individual neurons. The authors describe the scientific motivation for population analyses and the dimensionality reduction methods commonly applied to population activity. They also offer practical advice about selecting methods and interpreting their outputs.},
  copyright = {2014 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  language = {en}
}

@inproceedings{duncker2018Adv.NeuralInf.Process.Syst._TemporalAlignmentLatent,
  title = {Temporal Alignment and Latent {{Gaussian}} Process Factor Inference in Population Spike Trains},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Duncker, Lea and Sahani, Maneesh},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}}
}

@article{ebied2018MedEngPhys_EvaluationMatrixFactorisation,
  title = {Evaluation of Matrix Factorisation Approaches for Muscle Synergy Extraction},
  author = {Ebied, Ahmed and {Kinney-Lang}, Eli and Spyrou, Loukianos and Escudero, Javier},
  year = {2018},
  month = jul,
  journal = {Medical Engineering \& Physics},
  volume = {57},
  pages = {51--60},
  issn = {1873-4030},
  doi = {10.1016/j.medengphy.2018.04.003},
  abstract = {The muscle synergy concept provides a widely-accepted paradigm to break down the complexity of motor control. In order to identify the synergies, different matrix factorisation techniques have been used in a repertoire of fields such as prosthesis control and biomechanical and clinical studies. However, the relevance of these matrix factorisation techniques is still open for discussion since there is no ground truth for the underlying synergies. Here, we evaluate factorisation techniques and investigate the factors that affect the quality of estimated synergies. We compared commonly used matrix factorisation methods: Principal component analysis (PCA), Independent component analysis (ICA), Non-negative matrix factorization (NMF) and second-order blind identification (SOBI). Publicly available real data were used to assess the synergies extracted by each factorisation method in the classification of wrist movements. Synthetic datasets were utilised to explore the effect of muscle synergy sparsity, level of noise and number of channels on the extracted synergies. Results suggest that the sparse synergy model and a higher number of channels would result in better estimated synergies. Without dimensionality reduction, SOBI showed better results than other factorisation methods. This suggests that SOBI would be an alternative when a limited number of electrodes is available but its performance was still poor in that case. Otherwise, NMF had the best performance when the number of channels was higher than the number of synergies. Therefore, NMF would be the best method for muscle synergy extraction.},
  language = {eng},
  pmid = {29703696},
  keywords = {Algorithms,Electromyography,Humans,Independent component analysis,Matrix factorisation,Models; Biological,Movement,Muscle synergy,Muscles,Non-negative matrix factorisation,Principal component analysis,Principal Component Analysis,Second-order blind identification,Signal Processing; Computer-Assisted,Surface electromyogram}
}

@article{elsayed2017NatNeurosci_StructureNeuralPopulation,
  title = {Structure in Neural Population Recordings: An Expected Byproduct of Simpler Phenomena?},
  shorttitle = {Structure in Neural Population Recordings},
  author = {Elsayed, Gamaleldin F. and Cunningham, John P.},
  year = {2017},
  month = sep,
  journal = {Nature Neuroscience},
  volume = {20},
  number = {9},
  pages = {1310--1318},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4617},
  abstract = {To what extent are population-level results an expected byproduct of simpler structure already known to exist in single neurons? Conventional controls are insufficient to perform this critical investigation. The authors developed a methodological framework to test the significance of population-level studies and apply it to prefrontal and motor cortices.},
  copyright = {2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  language = {en},
  keywords = {Computational neuroscience,Statistical methods}
}

@article{gao2015CurrentOpinioninNeurobiology_SimplicityComplexityBrave,
  title = {On Simplicity and Complexity in the Brave New World of Large-Scale Neuroscience},
  author = {Gao, Peiran and Ganguli, Surya},
  year = {2015},
  month = jun,
  journal = {Current Opinion in Neurobiology},
  series = {Large-{{Scale Recording Technology}} (32)},
  volume = {32},
  pages = {148--155},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2015.04.003},
  abstract = {Technological advances have dramatically expanded our ability to probe multi-neuronal dynamics and connectivity in the brain. However, our ability to extract a simple conceptual understanding from complex data is increasingly hampered by the lack of theoretically principled data analytic procedures, as well as theoretical frameworks for how circuit connectivity and dynamics can conspire to generate emergent behavioral and cognitive functions. We review and outline potential avenues for progress, including new theories of high dimensional data analysis, the need to analyze complex artificial networks, and methods for analyzing entire spaces of circuit models, rather than one model at a time. Such interplay between experiments, data analysis and theory will be indispensable in catalyzing conceptual advances in the age of large-scale neuroscience.},
  language = {en}
}

@article{gao2017bioRxiv_TheoryMultineuronalDimensionality,
  title = {A Theory of Multineuronal Dimensionality, Dynamics and Measurement},
  author = {Gao, Peiran and Trautmann, Eric and Yu, Byron and Santhanam, Gopal and Ryu, Stephen and Shenoy, Krishna and Ganguli, Surya},
  year = {2017},
  month = nov,
  journal = {bioRxiv},
  pages = {214262},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/214262},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}In many experiments, neuroscientists tightly control behavior, record many trials, and obtain trial-averaged firing rates from hundreds of neurons in circuits containing billions of behaviorally relevant neurons. Di-mensionality reduction methods reveal a striking simplicity underlying such multi-neuronal data: they can be reduced to a low-dimensional space, and the resulting neural trajectories in this space yield a remarkably insightful dynamical portrait of circuit computation. This simplicity raises profound and timely conceptual questions. What are its origins and its implications for the complexity of neural dynamics? How would the situation change if we recorded more neurons? When, if at all, can we trust dynamical portraits obtained from measuring an infinitesimal fraction of task relevant neurons? We present a theory that answers these questions, and test it using physiological recordings from reaching monkeys. This theory reveals conceptual insights into how task complexity governs both neural dimensionality and accurate recovery of dynamic portraits, thereby providing quantitative guidelines for future large-scale experimental design.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  language = {en}
}

@article{gorard2015J.Educ.Soc.Behav.Sci._AbsoluteDeviationApproach,
  title = {An {{Absolute Deviation Approach}} to {{Assessing Correlation}}},
  author = {Gorard, Stephen},
  year = {2015},
  journal = {Journal of Education, Society and Behavioural Science},
  pages = {73--81},
  issn = {2456-981X},
  doi = {10.9734/BJESBS/2015/11381},
  language = {en-US},
  keywords = {correlation,Mean absolute deviation,quantitative methods initiative,statistical methods,the new statistics}
}

@article{happ2018J.Am.Stat.Assoc._MultivariateFunctionalPrincipal,
  title = {Multivariate {{Functional Principal Component Analysis}} for {{Data Observed}} on {{Different}} ({{Dimensional}}) {{Domains}}},
  author = {Happ, Clara and Greven, Sonja},
  year = {2018},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {113},
  number = {522},
  pages = {649--659},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2016.1273115},
  abstract = {Existing approaches for multivariate functional principal component analysis are restricted to data on the same one-dimensional interval. The presented approach focuses on multivariate functional data on different domains that may differ in dimension, such as functions and images. The theoretical basis for multivariate functional principal component analysis is given in terms of a Karhunen\textendash Lo\`eve Theorem. For the practically relevant case of a finite Karhunen\textendash Lo\`eve representation, a relationship between univariate and multivariate functional principal component analysis is established. This offers an estimation strategy to calculate multivariate functional principal components and scores based on their univariate counterparts. For the resulting estimators, asymptotic results are derived. The approach can be extended to finite univariate expansions in general, not necessarily orthonormal bases. It is also applicable for sparse functional data or data with measurement error. A flexible R implementation is available on CRAN. The new method is shown to be competitive to existing approaches for data observed on a common one-dimensional domain. The motivating application is a neuroimaging study, where the goal is to explore how longitudinal trajectories of a neuropsychological test score covary with FDG-PET brain scans at baseline. Supplementary material, including detailed proofs, additional simulation results, and software is available online.},
  keywords = {Dimension reduction,Functional data analysis,Image analysis,Multivariate functional data},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2016.1273115}
}

@article{jude2022undefined_RobustAlignmentCrosssession,
  title = {Robust Alignment of Cross-Session Recordings of Neural Population Activity by Behaviour via Unsupervised Domain Adaptation},
  author = {Jude, Justin and Perich, Matthew G. and Miller, Lee E. and Hennig, Matthias H.},
  year = {2022},
  month = feb,
  abstract = {Neural population activity relating to behaviour is assumed to be inherently low-dimensional despite the observed high dimensionality of data recorded using multi-electrode arrays. Therefore, predicting behaviour from neural population recordings has been shown to be most effective when using latent variable models. Over time however, the activity of single neurons can drift, and different neurons will be recorded due to movement of implanted neural probes. This means that a decoder trained to predict behaviour on one day performs worse when tested on a different day. On the other hand, evidence suggests that the latent dynamics underlying behaviour may be stable even over months and years. Based on this idea, we introduce a model capable of inferring behaviourally relevant latent dynamics from previously unseen data recorded from the same animal, without any need for decoder recalibration. We show that unsupervised domain adaptation combined with a sequential variational autoencoder, trained on several sessions, can achieve good generalisation to unseen data and correctly predict behaviour where conventional methods fail. Our results further support the hypothesis that behaviour-related neural dynamics are low-dimensional and stable over time, and will enable more effective and flexible use of brain computer interface technologies.},
  language = {en}
}

@article{kim2021IEEETrans.NeuralSyst.Rehabil.Eng._FindingKinematicsDrivenLatent,
  title = {Finding {{Kinematics}}-{{Driven Latent Neural States}} from {{Neuronal Population Activity}} for {{Motor Decoding}}},
  author = {Kim, Min-Ki and Sohn, Jeong-woo and Kim, Sung-Phil},
  year = {2021},
  journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  pages = {1--1},
  issn = {1558-0210},
  doi = {10.1109/TNSRE.2021.3114367},
  abstract = {While intracortical brain\textendash machine interfaces (BMIs) demonstrate feasibility to restore mobility to people with paralysis, it is still challenging to maintain high-performance decoding in clinical BMIs. One of the main obstacles for high-performance BMI is the noise-prone nature of traditional decoding methods that connect neural response explicitly with physical quantity, such as velocity. In contrast, the recent development of latent neural state model enables a robust readout of large-scale neuronal population activity contents. However, these latent neural states do not necessarily contain kinematic information useful for decoding. Therefore, this study proposes a new approach to finding kinematics-dependent latent factors by extracting latent factors' kinematics-dependent components using linear regression. We estimated these components from the population activity through nonlinear mapping. The proposed kinematics-dependent latent factors generate neural trajectories that discriminate latent neural states before and after the motion onset. We compared the decoding performance of the proposed analysis model with the results from other popular models. They are factor analysis (FA), Gaussian process factor analysis (GPFA), latent factor analysis via dynamical systems (LFADS), preferential subspace identification (PSID), and neuronal population firing rates. The proposed analysis model results in higher decoding accuracy than do the others ({$>$}17\% improvement on average). Our approach may pave a new way to extract latent neural states specific to kinematic information from motor cortices, potentially improving decoding performance for online intracortical BMIs.},
  keywords = {Decoding,factor analysis,Firing,intracortical brain–machine interface,Kinematics,kinematics-dependent latent factor,motor decoding,neural trajectory,Neurons,Noise measurement,Sociology,Statistics}
}

@article{kobak2016eLife_DemixedPrincipalComponent,
  title = {Demixed Principal Component Analysis of Neural Population Data},
  author = {Kobak, Dmitry and Brendel, Wieland and Constantinidis, Christos and Feierstein, Claudia E and Kepecs, Adam and Mainen, Zachary F and Qi, Xue-Lian and Romo, Ranulfo and Uchida, Naoshige and Machens, Christian K},
  editor = {{van Rossum}, Mark CW},
  year = {2016},
  month = apr,
  journal = {eLife},
  volume = {5},
  pages = {e10989},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.10989},
  abstract = {Neurons in higher cortical areas, such as the prefrontal cortex, are often tuned to a variety of sensory and motor variables, and are therefore said to display mixed selectivity. This complexity of single neuron responses can obscure what information these areas represent and how it is represented. Here we demonstrate the advantages of a new dimensionality reduction technique, demixed principal component analysis (dPCA), that decomposes population activity into a few components. In addition to systematically capturing the majority of the variance of the data, dPCA also exposes the dependence of the neural representation on task parameters such as stimuli, decisions, or rewards. To illustrate our method we reanalyze population data from four datasets comprising different species, different cortical areas and different experimental tasks. In each case, dPCA provides a concise way of visualizing the data that summarizes the task-dependent features of the population response in a single figure.},
  keywords = {dimensionality reduction,population activity,prefrontal cortex,principal component analysis}
}

@article{luczak2009Neuron_SpontaneousEventsOutline,
  title = {Spontaneous {{Events Outline}} the {{Realm}} of {{Possible Sensory Responses}} in {{Neocortical Populations}}},
  author = {Luczak, Artur and Barth{\'o}, Peter and Harris, Kenneth D.},
  year = {2009},
  month = may,
  journal = {Neuron},
  volume = {62},
  number = {3},
  pages = {413--425},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2009.03.014},
  abstract = {Neocortical assemblies produce complex activity patterns both in response to sensory stimuli and spontaneously without sensory input. To investigate the structure of these patterns, we recorded from populations of 40\textendash 100 neurons in auditory and somatosensory cortices of anesthetized and awake rats using silicon microelectrodes. Population spike time patterns were broadly conserved across multiple sensory stimuli and spontaneous events. Although individual neurons showed timing variations between stimuli, these were not sufficient to disturb a generally conserved sequential organization observed at the population level, lasting for approximately 100 ms with spiking reliability decaying progressively after event onset. Preserved constraints were also seen in population firing rate vectors, with vectors evoked by individual stimuli occupying subspaces of a larger but still constrained space outlined by the set of spontaneous events. These results suggest that population spike patterns are drawn from a limited ``vocabulary,'' sampled widely by spontaneous events but more narrowly by sensory responses.},
  language = {en},
  keywords = {SYSNEURO}
}

@article{mackevicius2019eLife_UnsupervisedDiscoveryTemporal,
  title = {Unsupervised Discovery of Temporal Sequences in High-Dimensional Datasets, with Applications to Neuroscience},
  author = {Mackevicius, Emily L and Bahle, Andrew H and Williams, Alex H and Gu, Shijie and Denisenko, Natalia I and Goldman, Mark S and Fee, Michale S},
  editor = {Colgin, Laura and Behrens, Timothy E},
  year = {2019},
  month = feb,
  journal = {eLife},
  volume = {8},
  pages = {e38471},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.38471},
  abstract = {Identifying low-dimensional features that describe large-scale neural recordings is a major challenge in neuroscience. Repeated temporal patterns (sequences) are thought to be a salient feature of neural dynamics, but are not succinctly captured by traditional dimensionality reduction techniques. Here, we describe a software toolbox\textemdash called seqNMF\textemdash with new methods for extracting informative, non-redundant, sequences from high-dimensional neural data, testing the significance of these extracted patterns, and assessing the prevalence of sequential structure in data. We test these methods on simulated data under multiple noise conditions, and on several real neural and behavioral data sets. In hippocampal data, seqNMF identifies neural sequences that match those calculated manually by reference to behavioral events. In songbird data, seqNMF discovers neural sequences in untutored birds that lack stereotyped songs. Thus, by identifying temporal structure directly from neural data, seqNMF enables dissection of complex neural circuits without relying on temporal references from stimuli or behavioral outputs.},
  keywords = {matrix factorization,sequence,unsupervised,Zebra finch}
}

@article{pandarinath2018Nat.Methods_InferringSingletrialNeural,
  title = {Inferring Single-Trial Neural Population Dynamics Using Sequential Auto-Encoders},
  author = {Pandarinath, Chethan and O'Shea, Daniel J. and Collins, Jasmine and Jozefowicz, Rafal and Stavisky, Sergey D. and Kao, Jonathan C. and Trautmann, Eric M. and Kaufman, Matthew T. and Ryu, Stephen I. and Hochberg, Leigh R. and Henderson, Jaimie M. and Shenoy, Krishna V. and Abbott, L. F. and Sussillo, David},
  year = {2018},
  month = oct,
  journal = {Nature Methods},
  volume = {15},
  number = {10},
  pages = {805--815},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/s41592-018-0109-9},
  abstract = {Neuroscience is experiencing a revolution in which simultaneous recording of thousands of neurons is revealing population dynamics that are not apparent from single-neuron responses. This structure is typically extracted from data averaged across many trials, but deeper understanding requires studying phenomena detected in single trials, which is challenging due to incomplete sampling of the neural population, trial-to-trial variability, and fluctuations in action potential timing. We introduce latent factor analysis via dynamical systems, a deep learning method to infer latent dynamics from single-trial neural spiking data. When applied to a variety of macaque and human motor cortical datasets, latent factor analysis via dynamical systems accurately predicts observed behavioral variables, extracts precise firing rate estimates of neural dynamics on single trials, infers perturbations to those dynamics that correlate with behavioral choices, and combines data from non-overlapping recording sessions spanning months to improve inference of underlying dynamics.},
  copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  language = {en}
}

@misc{sani2021undefined_WhereAllNonlinearity,
  title = {Where Is All the Nonlinearity: Flexible Nonlinear Modeling of Behaviorally Relevant Neural Dynamics Using Recurrent Neural Networks},
  shorttitle = {Where Is All the Nonlinearity},
  author = {Sani, Omid G. and Pesaran, Bijan and Shanechi, Maryam M.},
  year = {2021},
  month = sep,
  pages = {2021.09.03.458628},
  institution = {{bioRxiv}},
  doi = {10.1101/2021.09.03.458628},
  abstract = {Understanding the dynamical transformation of neural activity to behavior requires modeling this transformation while both dissecting its potential nonlinearities and dissociating and preserving its nonlinear behaviorally relevant neural dynamics, which remain unaddressed. We present RNN PSID, a nonlinear dynamic modeling method that enables flexible dissection of nonlinearities, dissociation and preferential learning of neural dynamics relevant to specific behaviors, and causal decoding. We first validate RNN PSID in simulations and then use it to investigate nonlinearities in monkey spiking and LFP activity across four tasks and different brain regions. Nonlinear RNN PSID successfully dissociated and preserved nonlinear behaviorally relevant dynamics, thus outperforming linear and non-preferential nonlinear learning methods in behavior decoding while reaching similar neural prediction. Strikingly, dissecting the nonlinearities with RNN PSID revealed that consistently across all tasks, summarizing the nonlinearity only in the mapping from the latent dynamics to behavior was largely sufficient for predicting behavior and neural activity. RNN PSID provides a novel tool to reveal new characteristics of nonlinear neural dynamics underlying behavior.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  language = {en}
}

@article{williams2018Neuron_UnsupervisedDiscoveryDemixed,
  title = {Unsupervised {{Discovery}} of {{Demixed}}, {{Low}}-{{Dimensional Neural Dynamics}} across {{Multiple Timescales}} through {{Tensor Component Analysis}}},
  author = {Williams, Alex H. and Kim, Tony Hyun and Wang, Forea and Vyas, Saurabh and Ryu, Stephen I. and Shenoy, Krishna V. and Schnitzer, Mark and Kolda, Tamara G. and Ganguli, Surya},
  year = {2018},
  month = jun,
  journal = {Neuron},
  volume = {98},
  number = {6},
  pages = {1099-1115.e8},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2018.05.015},
  abstract = {Perceptions, thoughts, and actions unfold over millisecond timescales, while learned behaviors can require many days to mature. While recent experimental advances enable large-scale and long-term neural recordings with high temporal fidelity, it remains a formidable challenge to extract unbiased and interpretable descriptions of how rapid single-trial circuit dynamics change slowly over many trials to mediate learning. We demonstrate a simple tensor component analysis (TCA) can meet this challenge by extracting three interconnected, low-dimensional descriptions of neural data: neuron factors, reflecting cell assemblies; temporal factors, reflecting rapid circuit dynamics mediating perceptions, thoughts, and actions within each trial; and trial factors, describing both long-term learning and trial-to-trial changes in cognitive state. We demonstrate the broad applicability of TCA by revealing insights into~diverse datasets derived from artificial neural networks, large-scale calcium imaging of rodent prefrontal cortex during maze navigation, and multielectrode recordings of macaque motor cortex during brain machine interface learning.},
  language = {en},
  keywords = {brain machine interfaces,dimensionality reduction,gain modulation,large-scale recordings,learning,motor control,navigation,neural data analysis,recurrent neural networks,single-trial analysis}
}

@article{yu2009J.Neurophysiol._GaussianProcessFactorAnalysis,
  title = {Gaussian-{{Process Factor Analysis}} for {{Low}}-{{Dimensional Single}}-{{Trial Analysis}} of {{Neural Population Activity}}},
  author = {Yu, Byron M. and Cunningham, John P. and Santhanam, Gopal and Ryu, Stephen I. and Shenoy, Krishna V. and Sahani, Maneesh},
  year = {2009},
  month = jul,
  journal = {Journal of Neurophysiology},
  volume = {102},
  number = {1},
  pages = {614--635},
  publisher = {{American Physiological Society}},
  issn = {0022-3077},
  doi = {10.1152/jn.90941.2008},
  abstract = {We consider the problem of extracting smooth, low-dimensional neural trajectories that summarize the activity recorded simultaneously from many neurons on individual experimental trials. Beyond the benefit of visualizing the high-dimensional, noisy spiking activity in a compact form, such trajectories can offer insight into the dynamics of the neural circuitry underlying the recorded activity. Current methods for extracting neural trajectories involve a two-stage process: the spike trains are first smoothed over time, then a static dimensionality-reduction technique is applied. We first describe extensions of the two-stage methods that allow the degree of smoothing to be chosen in a principled way and that account for spiking variability, which may vary both across neurons and across time. We then present a novel method for extracting neural trajectories\textemdash Gaussian-process factor analysis (GPFA)\textemdash which unifies the smoothing and dimensionality-reduction operations in a common probabilistic framework. We applied these methods to the activity of 61 neurons recorded simultaneously in macaque premotor and motor cortices during reach planning and execution. By adopting a goodness-of-fit metric that measures how well the activity of each neuron can be predicted by all other recorded neurons, we found that the proposed extensions improved the predictive ability of the two-stage methods. The predictive ability was further improved by going to GPFA. From the extracted trajectories, we directly observed a convergence in neural state during motor planning, an effect that was shown indirectly by previous studies. We then show how such methods can be a powerful tool for relating the spiking activity across a neural population to the subject's behavior on a single-trial basis. Finally, to assess how well the proposed methods characterize neural population activity when the underlying time course is known, we performed simulations that revealed that GPFA performed tens of percent better than the best two-stage method.}
}



@article{aitchison2021Nat.Neurosci._SynapticPlasticityBayesian,
  title = {Synaptic Plasticity as {{Bayesian}} Inference},
  author = {Aitchison, Laurence and Jegminat, Jannes and Menendez, Jorge Aurelio and Pfister, Jean-Pascal and Pouget, Alexandre and Latham, Peter E.},
  year = {2021},
  month = mar,
  journal = {Nature Neuroscience},
  pages = {1--7},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-021-00809-5},
  urldate = {2021-03-12},
  abstract = {Learning, especially rapid learning, is critical for survival. However, learning is hard; a large number of synaptic weights must be set based on noisy, often ambiguous, sensory information. In such a high-noise regime, keeping track of probability distributions over weights is the optimal strategy. Here we hypothesize that synapses take that strategy; in essence, when they estimate weights, they include error bars. They then use that uncertainty to adjust their learning rates, with more uncertain weights having higher learning rates. We also make a second, independent, hypothesis: synapses communicate their uncertainty by linking it to variability in postsynaptic potential size, with more uncertainty leading to more variability. These two hypotheses cast synaptic plasticity as a problem of Bayesian inference, and thus provide a normative view of learning. They generalize known learning rules, offer an explanation for the large variability in the size of postsynaptic potentials and make falsifiable experimental predictions.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/paper-pdfs/synaptic-plasticity/Aitchison2021Nature_Neuroscience-Synaptic_plasticity_as_Bayesian_inference.pdf}
}

@article{bi1998J.Neurosci._SynapticModificationsCultured,
  title = {Synaptic {{Modifications}} in {{Cultured Hippocampal Neurons}}: {{Dependence}} on {{Spike Timing}}, {{Synaptic Strength}}, and {{Postsynaptic Cell Type}}},
  shorttitle = {Synaptic {{Modifications}} in {{Cultured Hippocampal Neurons}}},
  author = {Bi, Guo-qiang and Poo, Mu-ming},
  year = {1998},
  month = dec,
  journal = {J. Neurosci.},
  volume = {18},
  number = {24},
  pages = {10464--10472},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.18-24-10464.1998},
  urldate = {2020-01-24},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/paper-pdfs/synaptic-plasticity/BiPoo1998J._Neurosci.-Synaptic_Modifications_in_Cultured_Hippocampal_Neurons.pdf}
}

@article{chen2015Nat.Neurosci._SubtypespecificPlasticityInhibitory,
  title = {Subtype-Specific Plasticity of Inhibitory Circuits in Motor Cortex during Motor Learning},
  author = {Chen, Simon X. and Kim, An Na and Peters, Andrew J. and Komiyama, Takaki},
  year = {2015},
  month = aug,
  journal = {Nature Neuroscience},
  volume = {18},
  number = {8},
  pages = {1109--1115},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4049},
  urldate = {2021-03-14},
  abstract = {This study identifies opposite changes in two main subtypes of inhibitory neurons in the mouse motor cortex during motor learning. With learning, the number of synapses made by somatostatin-expressing inhibitory neurons (SOM-IN) onto the distal dendritic branches of pyramidal neurons decreased, whereas the number of perisomatic contacts made by parvalbumin-positive cells increased. The authors also found that optogenetic disruption of SOM-IN activity resulted in impairment of learning-related dendritic spine reorganization and motor learning.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/paper-pdfs/motor-learning/Chen2015Nature_Neuroscience-Subtype-specific_plasticity_of_inhibitory_circuits_in_motor_cortex_during_motor.pdf;/Users/jamenendez11/Zotero/storage/H5K2DW5Q/nn.html}
}

@article{cichon2015Nature_BranchspecificDendriticCa,
  title = {Branch-Specific Dendritic {{Ca}} 2+ Spikes Cause Persistent Synaptic Plasticity},
  author = {Cichon, Joseph and Gan, Wen-Biao},
  year = {2015},
  month = apr,
  journal = {Nature},
  volume = {520},
  number = {7546},
  pages = {180--185},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature14251},
  urldate = {2021-03-14},
  abstract = {The brain has an extraordinary capacity for memory storage, but how it stores new information without disrupting previously acquired memories remains unknown. Here we show that different motor learning tasks induce dendritic Ca2+ spikes on different apical tuft branches of individual layer V pyramidal neurons in the mouse motor cortex. These task-related, branch-specific Ca2+ spikes cause long-lasting potentiation of postsynaptic dendritic spines active at the time of spike generation. When somatostatin-expressing interneurons are inactivated, different motor tasks frequently induce Ca2+ spikes on the same branches. On those branches, spines potentiated during one task are depotentiated when they are active seconds before Ca2+ spikes induced by another task. Concomitantly, increased neuronal activity and performance improvement after learning one task are disrupted when another task is learned. These findings indicate that dendritic-branch-specific generation of Ca2+ spikes is crucial for establishing long-lasting synaptic plasticity, thereby facilitating information storage associated with different learning experiences.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/paper-pdfs/motor-learning/CichonGan2015Nature-Branch-specific_dendritic_Ca_2+_spikes_cause_persistent_synaptic_plasticity.pdf;/Users/jamenendez11/Zotero/storage/A89S9G97/nature14251.html}
}

@book{dayan2001_TheoreticalNeuroscienceComputational,
  title = {Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems},
  shorttitle = {Theoretical Neuroscience},
  author = {Dayan, Peter and Abbott, Laurence F.},
  year = {2001},
  file = {/Users/jamenendez11/Zotero/storage/3QR3G5XU/ViewItemOverviewPage.html}
}

@article{dayan2011Neuron_NeuroplasticitySubservingMotor,
  title = {Neuroplasticity {{Subserving Motor Skill Learning}}},
  author = {Dayan, Eran and Cohen, Leonardo~G.},
  year = {2011},
  month = nov,
  journal = {Neuron},
  volume = {72},
  number = {3},
  pages = {443--454},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2011.10.008},
  urldate = {2020-07-22},
  abstract = {Recent years have seen significant progress in our understanding of the neural substrates of motor skill learning. Advances in neuroimaging provide new insight into functional reorganization associated with the acquisition, consolidation, and retention of motor skills. Plastic changes involving structural reorganization in gray and white matter architecture that occur over shorter time periods than previously thought have been documented as well. Data from experimental animals provided crucial information on plausible cellular and molecular substrates contributing to brain reorganization underlying skill acquisition in humans. Here, we review findings demonstrating functional and structural plasticity across different spatial and temporal scales that mediate motor skill learning while identifying converging areas of interest and possible avenues for future research.},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/paper-pdfs/synaptic-plasticity/DayanCohen2011Neuron-Neuroplasticity_Subserving_Motor_Skill_Learning.pdf;/Users/jamenendez11/Zotero/storage/I7LNYXBG/S0896627311009184.html}
}

@article{feulner2021PLOSComputationalBiology_NeuralManifoldPlasticity,
  title = {Neural Manifold under Plasticity in a Goal Driven Learning Behaviour},
  author = {Feulner, Barbara and Clopath, Claudia},
  year = {2021},
  month = feb,
  journal = {PLOS Computational Biology},
  volume = {17},
  number = {2},
  pages = {e1008621},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008621},
  urldate = {2021-03-06},
  abstract = {Neural activity is often low dimensional and dominated by only a few prominent neural covariation patterns. It has been hypothesised that these covariation patterns could form the building blocks used for fast and flexible motor control. Supporting this idea, recent experiments have shown that monkeys can learn to adapt their neural activity in motor cortex on a timescale of minutes, given that the change lies within the original low-dimensional subspace, also called neural manifold. However, the neural mechanism underlying this within-manifold adaptation remains unknown. Here, we show in a computational model that modification of recurrent weights, driven by a learned feedback signal, can account for the observed behavioural difference between within- and outside-manifold learning. Our findings give a new perspective, showing that recurrent weight changes do not necessarily lead to change in the neural manifold. On the contrary, successful learning is naturally constrained to a common subspace.},
  langid = {english},
  keywords = {Eigenvalues,Learning,Machine learning,Monkeys,Motor cortex,Neural networks,Neurons,Retraining},
  file = {/Users/jamenendez11/Library/CloudStorage/GoogleDrive-j.audi11@gmail.com/My Drive/paper-pdfs/synaptic-plasticity/FeulnerClopath2021PLOS_Computational_Biology-Neural_manifold_under_plasticity_in_a_goal_driven_learning_behaviour.pdf;/Users/jamenendez11/Zotero/storage/C3WX5AQX/article.html}
}

@article{feulner2022NatCommun_SmallCorrelatedChanges,
  title = {Small, Correlated Changes in Synaptic Connectivity May Facilitate Rapid Motor Learning},
  author = {Feulner, Barbara and Perich, Matthew G. and Chowdhury, Raeed H. and Miller, Lee E. and Gallego, Juan A. and Clopath, Claudia},
  year = {2022},
  month = sep,
  journal = {Nat Commun},
  volume = {13},
  number = {1},
  pages = {5163},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-32646-w},
  urldate = {2023-05-02},
  abstract = {Animals rapidly adapt their movements to external perturbations, a process paralleled by changes in neural activity in the motor cortex. Experimental studies suggest that these changes originate from altered inputs (Hinput) rather than from changes in local connectivity (Hlocal), as neural covariance is largely preserved during adaptation. Since measuring synaptic changes in vivo remains very challenging, we used a modular recurrent neural network to qualitatively test this interpretation. As expected, Hinput resulted in small activity changes and largely preserved covariance. Surprisingly given the presumed dependence of stable covariance on preserved circuit connectivity, Hlocal led to only slightly larger changes in activity and covariance, still within the range of experimental recordings. This similarity is due to Hlocal only requiring small, correlated connectivity changes for successful adaptation. Simulations of tasks that impose increasingly larger behavioural changes revealed a growing difference between Hinput and Hlocal, which could be exploited when designing future experiments.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Computational neuroscience,Learning and memory,Motor control,Neuroscience},
  file = {/Users/jamenendez11/Library/CloudStorage/GoogleDrive-j.audi11@gmail.com/My Drive/paper-pdfs/synaptic-plasticity/Feulner2022Nat_Commun-Small,_correlated_changes_in_synaptic_connectivity_may_facilitate_rapid_motor.pdf}
}

@misc{francioni2023_VectorizedInstructiveSignals,
  title = {Vectorized Instructive Signals in Cortical Dendrites during a Brain-Computer Interface Task},
  author = {Francioni, Valerio and Tang, Vincent D. and Brown, Norma J. and Toloza, Enrique H. S. and Harnett, Mark},
  year = {2023},
  month = nov,
  primaryclass = {New Results},
  pages = {2023.11.03.565534},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.11.03.565534},
  urldate = {2024-02-10},
  abstract = {Backpropagation of error is the most widely used learning algorithm in artificial neural networks, forming the backbone of modern machine learning and artificial intelligence1,2. Backpropagation provides a solution to the credit assignment problem by vectorizing an error signal tailored to individual neurons. Recent theoretical models have suggested that neural circuits could implement backpropagation-like learning by semi-independently processing feedforward and feedback information streams in separate dendritic compartments3{\textendash}7. This presents a compelling, but untested, hypothesis for how cortical circuits could solve credit assignment in the brain. We designed a neurofeedback brain-computer interface (BCI) task with an experimenter-defined reward function to evaluate the key requirements for dendrites to implement backpropagation-like learning. We trained mice to modulate the activity of two spatially intermingled populations (4 or 5 neurons each) of layer 5 pyramidal neurons in the retrosplenial cortex to rotate a visual grating towards a target orientation while we recorded GCaMP activity from somas and corresponding distal apical dendrites. We observed that the relative magnitudes of somatic versus dendritic signals could be predicted using the activity of the surrounding network and contained information about task-related variables that could serve as instructive signals, including reward and error. The signs of these putative teaching signals both depended on the causal role of individual neurons in the task and predicted changes in overall activity over the course of learning. These results provide the first biological evidence of a backpropagation-like solution to the credit assignment problem in the brain.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  langid = {english},
  file = {/Users/jamenendez11/Library/CloudStorage/GoogleDrive-j.audi11@gmail.com/My Drive/paper-pdfs/synaptic-plasticity/Francioni2023-Vectorized_instructive_signals_in_cortical_dendrites_during_a_brain-computer.pdf}
}

@article{hayashi-takagi2015Nature_LabellingOpticalErasure,
  title = {Labelling and Optical Erasure of Synaptic Memory Traces in the Motor Cortex},
  author = {{Hayashi-Takagi}, Akiko and Yagishita, Sho and Nakamura, Mayumi and Shirai, Fukutoshi and Wu, Yi I. and Loshbaugh, Amanda L. and Kuhlman, Brian and Hahn, Klaus M. and Kasai, Haruo},
  year = {2015},
  month = sep,
  journal = {Nature},
  volume = {525},
  number = {7569},
  pages = {333--338},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature15257},
  urldate = {2021-03-14},
  abstract = {Dendritic spines are the major loci of synaptic plasticity and are considered as possible structural correlates of memory. Nonetheless, systematic manipulation of specific subsets of spines in the cortex has been unattainable, and thus, the link between spines and memory has been correlational. We developed a novel synaptic optoprobe, AS-PaRac1 (activated synapse targeting photoactivatable Rac1), that can label recently potentiated spines specifically, and induce the selective shrinkage of AS-PaRac1-containing spines. In vivo imaging of AS-PaRac1 revealed that a motor learning task induced substantial synaptic remodelling in a small subset of neurons. The acquired motor learning was disrupted by the optical shrinkage of the potentiated spines, whereas it was not affected by the identical manipulation of spines evoked by a distinct motor task in the same cortical region. Taken together, our results demonstrate that a newly acquired motor skill depends on the formation of a task-specific dense synaptic ensemble.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/paper-pdfs/motor-learning/Hayashi-Takagi2015Nature-Labelling_and_optical_erasure_of_synaptic_memory_traces_in_the_motor_cortex.pdf;/Users/jamenendez11/Zotero/storage/NX7BC7UE/nature15257.html}
}

@article{hedrick2022NatNeurosci_LearningBindsNew,
  title = {Learning Binds New Inputs into Functional Synaptic Clusters via Spinogenesis},
  author = {Hedrick, Nathan G. and Lu, Zhongmin and Bushong, Eric and Singhi, Surbhi and Nguyen, Peter and Maga{\~n}a, Yessenia and Jilani, Sayyed and Lim, Byung Kook and Ellisman, Mark and Komiyama, Takaki},
  year = {2022},
  month = jun,
  journal = {Nat Neurosci},
  volume = {25},
  number = {6},
  pages = {726--737},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-022-01086-6},
  urldate = {2022-06-10},
  abstract = {Learning induces the formation of new excitatory synapses in the form of dendritic spines, but their functional properties remain unknown. Here, using longitudinal in vivo two-photon imaging and correlated electron microscopy of dendritic spines in the motor cortex of mice during motor learning, we describe a framework for the formation, survival and resulting function of new, learning-related spines. Specifically, our data indicate that the formation of new spines during learning is guided by the potentiation of functionally clustered preexisting spines exhibiting task-related activity during earlier sessions of learning. We present evidence that this clustered potentiation induces the local outgrowth of multiple filopodia from the nearby dendrite, locally sampling the adjacent neuropil for potential axonal partners, likely via targeting preexisting presynaptic boutons. Successful connections are then selected for survival based on co-activity with nearby task-related spines, ensuring that the new spine preserves functional clustering. The resulting locally coherent activity of new spines signals the learned movement. Furthermore, we found that a majority of new spines synapse with axons previously unrepresented in these dendritic domains. Thus, learning involves the binding of new information streams into functional synaptic clusters to subserve learned behaviors.},
  copyright = {2022 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Cortex,Neural circuits,Synaptic plasticity},
  file = {/Volumes/GoogleDrive/My Drive/paper-pdfs/synaptic-plasticity/Hedrick2022Nat_Neurosci-Learning_binds_new_inputs_into_functional_synaptic_clusters_via_spinogenesis.pdf;/Users/jamenendez11/Zotero/storage/TMN6KQLZ/s41593-022-01086-6.html}
}

@article{hiratani2022Adv.NeuralInf.Process.Syst._StabilityScalabilityNode,
  title = {On the {{Stability}} and {{Scalability}} of {{Node Perturbation Learning}}},
  author = {Hiratani, Naoki and Mehta, Yash and Lillicrap, Timothy and Latham, Peter E.},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {31929--31941},
  urldate = {2023-05-24},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/paper-pdfs/synaptic-plasticity/Hiratani2022Advances_in_Neural_Information_Processing_Systems-On_the_Stability_and_Scalability_of_Node_Perturbation_Learning.pdf}
}

@article{hoerzer2014Cereb.Cortex_EmergenceComplexComputational,
  title = {Emergence of {{Complex Computational Structures From Chaotic Neural Networks Through Reward-Modulated Hebbian Learning}}},
  author = {Hoerzer, Gregor M. and Legenstein, Robert and Maass, Wolfgang},
  year = {2014},
  month = mar,
  journal = {Cerebral Cortex},
  volume = {24},
  number = {3},
  pages = {677--690},
  issn = {1460-2199, 1047-3211},
  doi = {10.1093/cercor/bhs348},
  urldate = {2020-01-10},
  abstract = {This paper addresses the question how generic microcircuits of neurons in different parts of the cortex can attain and maintain different computational specializations. We show that if stochastic variations in the dynamics of local microcircuits are correlated with signals related to functional improvements of the brain (e.g. in the control of behavior), the computational operation of these microcircuits can become optimized for specific tasks such as the generation of specific periodic signals and task-dependent routing of information. Furthermore, we show that working memory can autonomously emerge through reward-modulated Hebbian learning, if needed for specific tasks. Altogether, our results suggest that reward-modulated synaptic plasticity can not only optimize the network parameters for specific computational tasks, but also initiate a functional rewiring that re-programs microcircuits, thereby generating diverse computational functions in different generic cortical microcircuits. On a more general level, this work provides a new perspective for a standard model for computations in generic cortical microcircuits (liquid computing model). It shows that the arguably most problematic assumption of this model, the postulate of a teacher that trains neural readouts through supervised learning, can be eliminated. We show that generic networks of neurons can learn numerous biologically relevant computations through trial and error.},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/paper-pdfs/synaptic-plasticity/Hoerzer2014Cerebral_Cortex-Emergence_of_Complex_Computational_Structures_From_Chaotic_Neural_Networks.pdf}
}

@article{jabri1992IEEETrans.NeuralNetw._WeightPerturbationOptimal,
  title = {Weight Perturbation: An Optimal Architecture and Learning Technique for Analog {{VLSI}} Feedforward and Recurrent Multilayer Networks},
  shorttitle = {Weight Perturbation},
  author = {Jabri, M. and Flower, B.},
  year = {1992},
  month = jan,
  journal = {IEEE Transactions on Neural Networks},
  volume = {3},
  number = {1},
  pages = {154--157},
  issn = {1941-0093},
  doi = {10.1109/72.105429},
  abstract = {Previous work on analog VLSI implementation of multilayer perceptrons with on-chip learning has mainly targeted the implementation of algorithms such as back-propagation. Although back-propagation is efficient, its implementation in analog VLSI requires excessive computational hardware. It is shown that using gradient descent with direct approximation of the gradient instead of back-propagation is more economical for parallel analog implementations. It is shown that this technique (which is called 'weight perturbation') is suitable for multilayer recurrent networks as well. A discrete level analog implementation showing the training of an XOR network as an example is presented.{$<>$}},
  keywords = {Analog computers,analog VLSI implementation,direct approximation,feedforward networks,Finite difference methods,gradient descent,Hardware,learning systems,learning technique,multilayer perceptrons,Multilayer perceptrons,Network-on-a-chip,neural nets,Neurons,Nonhomogeneous media,optimal architecture,parallel analog implementations,perturbation theory,Power generation economics,recurrent multilayer networks,Very large scale integration,VLSI,weight perturbation,Wires,XOR network},
  file = {/Volumes/GoogleDrive/My Drive/paper-pdfs/machine-learning/JabriFlower1992IEEE_Transactions_on_Neural_Networks-Weight_perturbation.pdf;/Users/jamenendez11/Zotero/storage/7D5E8THT/105429.html}
}

@article{kleim2002NeurobiologyofLearningandMemory_MotorLearningDependentSynaptogenesis,
  title = {Motor {{Learning-Dependent Synaptogenesis Is Localized}} to {{Functionally Reorganized Motor Cortex}}},
  author = {Kleim, Jeffrey A. and Barbay, Scott and Cooper, Natalie R. and Hogg, Theresa M. and Reidel, Chelsea N. and Remple, Michael S. and Nudo, Randolph J.},
  year = {2002},
  month = jan,
  journal = {Neurobiology of Learning and Memory},
  volume = {77},
  number = {1},
  pages = {63--77},
  issn = {1074-7427},
  doi = {10.1006/nlme.2000.4004},
  urldate = {2020-07-22},
  abstract = {The regional specificity and functional significance of learning-dependent synaptogenesis within physiologically defined regions of the adult motor cortex are described. In comparison to rats in a motor activity control group, rats trained on a skilled reaching task exhibited an areal expansion of wrist and digit movement representations within the motor cortex. No expansion of hindlimb representations was seen. This functional reorganization was restricted to the caudal forelimb area, as no differences in the topography of movement representations were observed within the rostral forelimb area. Paralleling the physiological changes, trained animals also had significantly more synapses per neuron than controls within layer V of the caudal forelimb area. No differences in the number of synapses per neuron were found in either the rostral forelimb or hindlimb areas. This is the first demonstration of the co-occurrence of functional and structural plasticity within the same cortical regions and provides strong evidence that synapse formation may play a role in supporting learning-dependent changes in cortical function.},
  langid = {english},
  keywords = {intracortical microstimulation,motor cortex,motor learning,plasticity,rat.,synaptogenesis},
  file = {/Volumes/GoogleDrive/My Drive/paper-pdfs/synaptic-plasticity/Kleim2002Neurobiology_of_Learning_and_Memory-Motor_Learning-Dependent_Synaptogenesis_Is_Localized_to_Functionally.pdf;/Users/jamenendez11/Zotero/storage/HLW25TRH/S1074742700940048.html}
}

@article{kleim2004J.Neurosci._CorticalSynaptogenesisMotor,
  title = {Cortical {{Synaptogenesis}} and {{Motor Map Reorganization Occur}} during {{Late}}, {{But Not Early}}, {{Phase}} of {{Motor Skill Learning}}},
  author = {Kleim, Jeffrey A. and Hogg, Theresa M. and VandenBerg, Penny M. and Cooper, Natalie R. and Bruneau, Rochelle and Remple, Michael},
  year = {2004},
  month = jan,
  journal = {J. Neurosci.},
  volume = {24},
  number = {3},
  pages = {628--633},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3440-03.2004},
  urldate = {2020-03-13},
  abstract = {Extensive motor skill training induces reorganization of movement representations and synaptogenesis within adult motor cortex. Motor skill does not, however, develop uniformly across training sessions. It is characterized by an initial fast phase, followed by a later slow phase of learning. How cortical plasticity emerges during these phases is unknown. Here, we examine motor map topography and synapse number within rat motor cortex during the early and late phases of motor learning. Adult rats were placed in either a skilled or unskilled reaching condition (SRC and URC, respectively) for 3, 7, or 10 d. Intracortical microstimulation of layer V was used to determine the topography of forelimb movement representations within caudal forelimb area of motor cortex contralateral to the trained paw. Quantitative electron microscopy was used to measure the number of synapses per neuron within layer V. SRC animals showed significant increases in reaching accuracy after 3, 7, and 10 d of training. In comparison with URC animals, SRC animals had significantly larger distal forelimb representations after 10 d of training only. Furthermore, SRC animals had significantly more synapses per neuron than URC animals after 7 and 10 d of training. These results show that both motor map reorganization and synapse formation occur during the late phase of skill learning. Furthermore, synaptogenesis precedes map reorganization. We propose that motor map reorganization and synapse formation do not contribute to the initial acquisition of motor skills but represent the consolidation of motor skill that occurs during late stages of training.},
  chapter = {Behavioral/Systems/Cognitive},
  copyright = {Copyright {\copyright} 2004 Society for Neuroscience 0270-6474/04/24628-06.00/0},
  langid = {english},
  pmid = {14736848},
  keywords = {intracortical microstimulation,learning,motor cortex,motor map,plasticity,rat,synaptogenesis},
  file = {/Users/jamenendez11/Library/CloudStorage/GoogleDrive-j.audi11@gmail.com/My Drive/paper-pdfs/synaptic-plasticity/Kleim2004J._Neurosci.-Cortical_Synaptogenesis_and_Motor_Map_Reorganization_Occur_during_Late,_But_Not.pdf}
}

@article{legenstein2008PLOSComputationalBiology_LearningTheoryRewardModulated,
  title = {A {{Learning Theory}} for {{Reward-Modulated Spike-Timing-Dependent Plasticity}} with {{Application}} to {{Biofeedback}}},
  author = {Legenstein, Robert and Pecevski, Dejan and Maass, Wolfgang},
  year = {2008},
  month = oct,
  journal = {PLOS Computational Biology},
  volume = {4},
  number = {10},
  pages = {e1000180},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000180},
  urldate = {2020-08-07},
  abstract = {Reward-modulated spike-timing-dependent plasticity (STDP) has recently emerged as a candidate for a learning rule that could explain how behaviorally relevant adaptive changes in complex networks of spiking neurons could be achieved in a self-organizing manner through local synaptic plasticity. However, the capabilities and limitations of this learning rule could so far only be tested through computer simulations. This article provides tools for an analytic treatment of reward-modulated STDP, which allows us to predict under which conditions reward-modulated STDP will achieve a desired learning effect. These analytical results imply that neurons can learn through reward-modulated STDP to classify not only spatial but also temporal firing patterns of presynaptic neurons. They also can learn to respond to specific presynaptic firing patterns with particular spike patterns. Finally, the resulting learning theory predicts that even difficult credit-assignment problems, where it is very hard to tell which synaptic weights should be modified in order to increase the global reward for the system, can be solved in a self-organizing manner through reward-modulated STDP. This yields an explanation for a fundamental experimental result on biofeedback in monkeys by Fetz and Baker. In this experiment monkeys were rewarded for increasing the firing rate of a particular neuron in the cortex and were able to solve this extremely difficult credit assignment problem. Our model for this experiment relies on a combination of reward-modulated STDP with variable spontaneous firing activity. Hence it also provides a possible functional explanation for trial-to-trial variability, which is characteristic for cortical networks of neurons but has no analogue in currently existing artificial computing systems. In addition our model demonstrates that reward-modulated STDP can be applied to all synapses in a large recurrent neural network without endangering the stability of the network dynamics.},
  langid = {english},
  keywords = {Action potentials,Computer modeling,Learning,Membrane potential,Neuronal plasticity,Neurons,Synapses,Synaptic plasticity},
  file = {/Users/jamenendez11/Zotero/storage/TKM82ZNV/Legenstein et al. - 2008 - A Learning Theory for Reward-Modulated Spike-Timin.pdf;/Users/jamenendez11/Zotero/storage/QNJNR3Z4/article.html}
}

@article{legenstein2010J.Neurosci._RewardModulatedHebbianLearning,
  title = {A {{Reward-Modulated Hebbian Learning Rule Can Explain Experimentally Observed Network Reorganization}} in a {{Brain Control Task}}},
  author = {Legenstein, Robert and Chase, Steven M. and Schwartz, Andrew B. and Maass, Wolfgang},
  year = {2010},
  month = jun,
  journal = {J. Neurosci.},
  volume = {30},
  number = {25},
  pages = {8400--8410},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4284-09.2010},
  urldate = {2020-03-13},
  abstract = {It has recently been shown in a brain{\textendash}computer interface experiment that motor cortical neurons change their tuning properties selectively to compensate for errors induced by displaced decoding parameters. In particular, it was shown that the three-dimensional tuning curves of neurons whose decoding parameters were reassigned changed more than those of neurons whose decoding parameters had not been reassigned. In this article, we propose a simple learning rule that can reproduce this effect. Our learning rule uses Hebbian weight updates driven by a global reward signal and neuronal noise. In contrast to most previously proposed learning rules, this approach does not require extrinsic information to separate noise from signal. The learning rule is able to optimize the performance of a model system within biologically realistic periods of time under high noise levels. Furthermore, when the model parameters are matched to data recorded during the brain{\textendash}computer interface learning experiments described above, the model produces learning effects strikingly similar to those found in the experiments.},
  chapter = {Articles},
  copyright = {Copyright {\copyright} 2010 the authors 0270-6474/10/308400-11\$15.00/0},
  langid = {english},
  pmid = {20573887},
  file = {/Users/jamenendez11/Library/CloudStorage/GoogleDrive-j.audi11@gmail.com/My Drive/paper-pdfs/bmi/Legenstein2010J._Neurosci.-A_Reward-Modulated_Hebbian_Learning_Rule_Can_Explain_Experimentally_Observed.pdf;/Users/jamenendez11/Zotero/storage/GAWY7YC5/8400.html}
}

@article{lillicrap2020Nat.Rev.Neurosci._BackpropagationBrain,
  title = {Backpropagation and the Brain},
  author = {Lillicrap, Timothy P. and Santoro, Adam and Marris, Luke and Akerman, Colin J. and Hinton, Geoffrey},
  year = {2020},
  month = jun,
  journal = {Nature Reviews Neuroscience},
  volume = {21},
  number = {6},
  pages = {335--346},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-020-0277-3},
  urldate = {2020-07-22},
  abstract = {During learning, the brain modifies synapses to improve behaviour. In the cortex, synapses are embedded within multilayered networks, making it difficult to determine the effect of an individual synaptic modification on the behaviour of the system. The backpropagation algorithm solves this problem in deep artificial neural networks, but historically it has been viewed as biologically problematic. Nonetheless, recent developments in neuroscience and the successes of artificial neural networks have reinvigorated interest in whether backpropagation offers insights for understanding learning in the cortex. The backpropagation algorithm learns quickly by computing synaptic updates using feedback connections to deliver error signals. Although feedback connections are ubiquitous in the cortex, it is difficult to see how they could deliver the error signals required by strict formulations of backpropagation. Here we build on past and recent developments to argue that feedback connections may instead induce neural activities whose differences can be used to locally approximate these signals and hence drive effective learning in deep networks in the brain.},
  copyright = {2020 Springer Nature Limited},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/paper-pdfs/synaptic-plasticity/Lillicrap2020Nature_Reviews_Neuroscience-Backpropagation_and_the_brain.pdf;/Users/jamenendez11/Zotero/storage/QIQ79WSV/s41583-020-0277-3.html}
}

@article{loewenstein2006PNAS_OperantMatchingGeneric,
  title = {Operant Matching Is a Generic Outcome of Synaptic Plasticity Based on the Covariance between Reward and Neural Activity},
  author = {Loewenstein, Yonatan and Seung, H. Sebastian},
  year = {2006},
  month = oct,
  journal = {PNAS},
  volume = {103},
  number = {41},
  pages = {15224--15229},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0505220103},
  urldate = {2020-08-09},
  abstract = {The probability of choosing an alternative in a long sequence of repeated choices is proportional to the total reward derived from that alternative, a phenomenon known as Herrnstein's matching law. This behavior is remarkably conserved across species and experimental conditions, but its underlying neural mechanisms still are unknown. Here, we propose a neural explanation of this empirical law of behavior. We hypothesize that there are forms of synaptic plasticity driven by the covariance between reward and neural activity and prove mathematically that matching is a generic outcome of such plasticity. Two hypothetical types of synaptic plasticity, embedded in decision-making neural network models, are shown to yield matching behavior in numerical simulations, in accord with our general theorem. We show how this class of models can be tested experimentally by making reward not only contingent on the choices of the subject but also directly contingent on fluctuations in neural activity. Maximization is shown to be a generic outcome of synaptic plasticity driven by the sum of the covariances between reward and all past neural activities.},
  chapter = {Biological Sciences},
  copyright = {{\copyright} 2006 by The National Academy of Sciences of the USA},
  langid = {english},
  pmid = {17008410},
  keywords = {decision making,neuroeconomics,rational choice theory,reinforcement learning},
  file = {/Volumes/GoogleDrive/My Drive/paper-pdfs/synaptic-plasticity/LoewensteinSeung2006PNAS-Operant_matching_is_a_generic_outcome_of_synaptic_plasticity_based_on_the.pdf;/Users/jamenendez11/Zotero/storage/RUW3S78N/15224.html}
}

@article{mazzoni1991PNAS_MoreBiologicallyPlausible,
  title = {A More Biologically Plausible Learning Rule for Neural Networks.},
  author = {Mazzoni, P. and Andersen, R. A. and Jordan, M. I.},
  year = {1991},
  month = may,
  journal = {PNAS},
  volume = {88},
  number = {10},
  pages = {4433--4437},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.88.10.4433},
  urldate = {2020-04-29},
  abstract = {Many recent studies have used artificial neural network algorithms to model how the brain might process information. However, back-propagation learning, the method that is generally used to train these networks, is distinctly "unbiological." We describe here a more biologically plausible learning rule, using reinforcement learning, which we have applied to the problem of how area 7a in the posterior parietal cortex of monkeys might represent visual space in head-centered coordinates. The network behaves similarly to networks trained by using back-propagation and to neurons recorded in area 7a. These results show that a neural network does not require back propagation to acquire biologically interesting properties.},
  chapter = {Research Article},
  langid = {english},
  pmid = {1903542},
  file = {/Volumes/GoogleDrive/My Drive/paper-pdfs/machine-learning/Mazzoni1991PNAS-A_more_biologically_plausible_learning_rule_for_neural_networks.pdf}
}

@article{peters2017Annu.Rev.Neurosci._LearningRodentMotor,
  title = {Learning in the {{Rodent Motor Cortex}}},
  author = {Peters, Andrew J. and Liu, Haixin and Komiyama, Takaki},
  year = {2017},
  journal = {Annual Review of Neuroscience},
  volume = {40},
  number = {1},
  pages = {77--97},
  doi = {10.1146/annurev-neuro-072116-031407},
  urldate = {2020-07-21},
  abstract = {The motor cortex is far from a stable conduit for motor commands and instead undergoes significant changes during learning. An understanding of motor cortex plasticity has been advanced greatly using rodents as experimental animals. Two major focuses of this research have been on the connectivity and activity of the motor cortex. The motor cortex exhibits structural changes in response to learning, and substantial evidence has implicated the local formation and maintenance of new synapses as crucial substrates of motor learning. This synaptic reorganization translates into changes in spiking activity, which appear to result in a modification and refinement of the relationship between motor cortical activity and movement. This review presents the progress that has been made using rodents to establish the motor cortex as an adaptive structure that supports motor learning.},
  pmid = {28375768},
  file = {/Volumes/GoogleDrive/My Drive/paper-pdfs/motor-learning/Peters2017Annual_Review_of_Neuroscience-Learning_in_the_Rodent_Motor_Cortex.pdf}
}

@article{raman2019PNAS_FundamentalBoundsLearning,
  title = {Fundamental Bounds on Learning Performance in Neural Circuits},
  author = {Raman, Dhruva Venkita and Rotondo, Adriana Perez and O'Leary, Timothy},
  year = {2019},
  month = may,
  journal = {PNAS},
  volume = {116},
  number = {21},
  pages = {10537--10546},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1813416116},
  urldate = {2020-07-22},
  abstract = {How does the size of a neural circuit influence its learning performance? Larger brains tend to be found in species with higher cognitive function and learning ability. Intuitively, we expect the learning capacity of a neural circuit to grow with the number of neurons and synapses. We show how adding apparently redundant neurons and connections to a network can make a task more learnable. Consequently, large neural circuits can either devote connectivity to generating complex behaviors or exploit this connectivity to achieve faster and more precise learning of simpler behaviors. However, we show that in a biologically relevant setting where synapses introduce an unavoidable amount of noise, there is an optimal size of network for a given task. Above the optimal network size, the addition of neurons and synaptic connections starts to impede learning performance. This suggests that the size of brain circuits may be constrained by the need to learn efficiently with unreliable synapses and provides a hypothesis for why some neurological learning deficits are associated with hyperconnectivity. Our analysis is independent of specific learning rules and uncovers fundamental relationships between learning rate, task performance, network size, and intrinsic noise in neural circuits.},
  chapter = {PNAS Plus},
  copyright = {Copyright {\copyright} 2019 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
  langid = {english},
  pmid = {31061133},
  keywords = {artificial intelligence,learning,neural network,optimization,synaptic plasticity},
  file = {/Users/jamenendez11/Library/CloudStorage/GoogleDrive-j.audi11@gmail.com/My Drive/paper-pdfs/synaptic-plasticity/Raman2019PNAS-Fundamental_bounds_on_learning_performance_in_neural_circuits.pdf;/Users/jamenendez11/Zotero/storage/PVSUZEDJ/10537.html}
}

@article{rioult-pedotti1998Nat.Neurosci._StrengtheningHorizontalCortical,
  title = {Strengthening of Horizontal Cortical Connections Following Skill Learning},
  author = {{Rioult-Pedotti}, Mengia-S. and Friedman, Daniel and Hess, Grzegorz and Donoghue, John P.},
  year = {1998},
  month = jul,
  journal = {Nature Neuroscience},
  volume = {1},
  number = {3},
  pages = {230--234},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/678},
  urldate = {2020-07-22},
  abstract = {Learning a new motor skill requires an alteration in the spatiotemporal pattern of muscle activation. Motor areas of cerebral neocortex are thought to be involved in this type of learning, possibly by functional reorganization of cortical connections. Here we show that skill learning is accompanied by changes in the strength of connections within adult rat primary motor cortex (M1). Rats were trained for three or five days in a skilled reaching task with one forelimb, after which slices of motor cortex were examined to determine the effect of training on the strength of horizontal intracortical connections in layer II/III. The amplitude of field potentials in the forelimb region contralateral to the trained limb was significantly increased relative to the opposite 'untrained' hemisphere. No differences were seen in the hindlimb region. Moreover, the amount of long-term potentiation (LTP) that could be induced in trained M1 was less than in controls, suggesting that the effect of training was at least partly due to LTP-like mechanisms. These data represent the first direct evidence that plasticity of intracortical connections is associated with learning a new motor skill.},
  copyright = {1998 Nature America Inc.},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/paper-pdfs/motor-learning/Rioult-Pedotti1998Nature_Neuroscience-Strengthening_of_horizontal_cortical_connections_following_skill_learning.pdf;/Users/jamenendez11/Zotero/storage/H976D9AZ/nn0798_230.html}
}

@article{song2005PLOSBiology_HighlyNonrandomFeatures,
  title = {Highly {{Nonrandom Features}} of {{Synaptic Connectivity}} in {{Local Cortical Circuits}}},
  author = {Song, Sen and Sj{\"o}str{\"o}m, Per Jesper and Reigl, Markus and Nelson, Sacha and Chklovskii, Dmitri B.},
  year = {2005},
  month = mar,
  journal = {PLOS Biology},
  volume = {3},
  number = {3},
  pages = {e68},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.0030068},
  urldate = {2020-07-22},
  abstract = {How different is local cortical circuitry from a random network? To answer this question, we probed synaptic connections with several hundred simultaneous quadruple whole-cell recordings from layer 5 pyramidal neurons in the rat visual cortex. Analysis of this dataset revealed several nonrandom features in synaptic connectivity. We confirmed previous reports that bidirectional connections are more common than expected in a random network. We found that several highly clustered three-neuron connectivity patterns are overrepresented, suggesting that connections tend to cluster together. We also analyzed synaptic connection strength as defined by the peak excitatory postsynaptic potential amplitude. We found that the distribution of synaptic connection strength differs significantly from the Poisson distribution and can be fitted by a lognormal distribution. Such a distribution has a heavier tail and implies that synaptic weight is concentrated among few synaptic connections. In addition, the strengths of synaptic connections sharing pre- or postsynaptic neurons are correlated, implying that strong connections are even more clustered than the weak ones. Therefore, the local cortical network structure can be viewed as a skeleton of stronger connections in a sea of weaker ones. Such a skeleton is likely to play an important role in network dynamics and should be investigated further.},
  langid = {english},
  keywords = {Action potentials,Axons,Excitatory postsynaptic potentials,Network motifs,Network reciprocity,Neural networks,Neuronal dendrites,Neurons},
  file = {/Volumes/GoogleDrive/My Drive/paper-pdfs/synaptic-plasticity/Song2005PLOS_Biology-Highly_Nonrandom_Features_of_Synaptic_Connectivity_in_Local_Cortical_Circuits.pdf;/Users/jamenendez11/Zotero/storage/B4338NXE/article.html}
}
